# 12.ceph

# 12.1.部署ceph集群

### 12.1.1.配置阿里ceph源

在所有节点执行如下操作

```
cat >/etc/yum.repos.d/ceph.repo<<EOF
[ceph]
name=ceph
baseurl=http://mirrors.aliyun.com/ceph/rpm-jewel/el7/x86_64/
gpgcheck=0
priority=1

[ceph-noarch]
name=cephnoarch
baseurl=http://mirrors.aliyun.com/ceph/rpm-jewel/el7/noarch/
gpgcheck=0
priority=1

[ceph-source]
name=Ceph source packages
baseurl=http://mirrors.163.com/ceph/rpm-jewel/el7/SRPMS
enabled=0
gpgcheck=1
type=rpm-md
gpgkey=http://mirrors.163.com/ceph/keys/release.asc
priority=1
EOF
yum makecache

```

### 12.1.2.在master节点安装ceph-deploy

```
yum install -y ceph-deploy

```

### 12.1.3.配置master节点ssh登录slave节点

安装之后需要配置admin节点可以ssh无密码登录每个node节点和测试节点，用户需要有sudo权限

```
# 在每一个node节点执行
useradd ceph
echo 'ceph' | passwd --stdin ceph
echo "ceph ALL = (root) NOPASSWD:ALL" > /etc/sudoers.d/ceph
chmod 0440 /etc/sudoers.d/ceph
# 配置sshd可以使用password登录
sed -i 's/PasswordAuthentication no/PasswordAuthentication yes/' /etc/ssh/sshd_config
systemctl reload sshd
# 配置sudo不需要tty
sed -i 's/Default requiretty/#Default requiretty/' /etc/sudoers

# 在所有节点配置hosts
cat >>/etc/hosts<<EOF
172.16.24.194 softeng-cn-hz-2c8g-node001
172.16.24.195 softeng-cn-hz-2c8g-node002
172.16.24.196 softeng-cn-hz-2c8g-node003
EOF

# 在admin节点执行
# 创建ceph用户，配置sshkey登录
# 由于lab1节点作为node节点时已经创建过ceph用户
# 第一条命令可能会出错，忽略即可
useradd ceph
su - ceph
ssh-keygen
ssh-copy-id ceph@softeng-cn-hz-2c8g-node001
ssh-copy-id ceph@softeng-cn-hz-2c8g-node002
ssh-copy-id ceph@softeng-cn-hz-2c8g-node003

```

### 12.1.4.在admin节点创建集群

```
# 不要使用sudo也不要使用root用户运行如下的命令
su - ceph
mkdir cluster
cd cluster
# 创建node001为monitor
ceph-deploy new softeng-cn-hz-2c8g-node001

# 查看配置文件
ls -l

# 配置ceph.conf
[global]
...
# 如果有多个网卡，应该配置如下选项，
# public network是公共网络，负责集群对外提供服务的流量
# cluster network是集群网络，负载集群中数据复制传输通信等
# 本次实验使用同一块网卡，生境环境建议分别使用一块网卡
[global]
fsid = 2d4ed573-51dc-44a1-ac49-1bea2231c379
mon_initial_members = softeng-cn-hz-2c8g-node001
mon_host = 172.16.24.194
auth_cluster_required = cephx
auth_service_required = cephx
auth_client_required = cephx
osd_pool_default_size = 2
mon_clock_drift_allowed = 2
public_network = 172.16.24.0/24
cluster_network = 172.16.24.0/24

# 安装 ceph 包
# 如果按照官方文档安装方法 会重新配置安装官方ceph源
# 由于网络问题，安装可能会出错，需要多次执行
# ceph-deploy install 其实只是会安装 ceph ceph-radosgw 两个包
# ceph-deploy install softeng-cn-hz-2c8g-node001 softeng-cn-hz-2c8g-node002 softeng-cn-hz-2c8g-node003
# 推荐使用阿里源安装，因为使用ceph-deploy安装会很慢
# 使用如下命令手动安装包，替代官方的 ceph-deploy install 命令
# 如下操作在所有node节点上执行
sudo yum install -y ceph ceph-radosgw

# 部署monitor和生成keys
ceph-deploy mon create-initial
ls -l *.keyring

# 复制文件到node节点
ceph-deploy admin softeng-cn-hz-2c8g-node001 softeng-cn-hz-2c8g-node002 softeng-cn-hz-2c8g-node003

# 部署manager （luminous+）12及以后的版本需要部署
# 本次部署 jewel 版本 ，不需要执行如下命令
# ceph-deploy mgr create lab1

sudo mkdir /var/local/osd
sudo chown -R ceph:ceph /var/local/osd


# 添加osd 以磁盘方式
# 本次实验采用此种方法
# sdb 为虚拟机添加的磁盘设置名
ceph-deploy osd create softeng-cn-hz-2c8g-node001:/var/local/osd softeng-cn-hz-2c8g-node002:/var/local/osd softeng-cn-hz-2c8g-node003:/var/local/osd


# 添加osd 以文件目录方式
ceph-deploy osd prepare softeng-cn-hz-2c8g-node001:/var/local/osd softeng-cn-hz-2c8g-node002:/var/local/osd  softeng-cn-hz-2c8g-node003:/var/local/osd 
ceph-deploy osd activate softeng-cn-hz-2c8g-node001:/var/local/osd softeng-cn-hz-2c8g-node002:/var/local/osd  softeng-cn-hz-2c8g-node003:/var/local/osd 

# 查看状态
ssh lab1 sudo ceph health
ssh lab1 sudo ceph -s

```

### 12.1.5.清理集群

```
# 如果安装过程出错，使用如下命令清理之后重新开始
ceph-deploy purge softeng-cn-hz-2c8g-node001 softeng-cn-hz-2c8g-node002 softeng-cn-hz-2c8g-node003
ceph-deploy purgedata softeng-cn-hz-2c8g-node001 softeng-cn-hz-2c8g-node002 softeng-cn-hz-2c8g-node003
ceph-deploy forgetkeys
rm ceph.*

```