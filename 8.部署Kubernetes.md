# 8.部署Kubernetes

## 8.1.Kubernetes部署利器kubeadm


### 8.1.1.基本介绍

kubeadm的目的是让用户能够使用如下两条命令来简单的部署kubernetes集群

```
# 创建一个 Master 节点
$ kubeadm init

# 将一个 Node 节点加入到当前集群中
$ kubeadm join <Master 节点的 IP 和端口 >

```

### 8.1.2.开始部署

原文<https://yq.aliyun.com/articles/626118>

* 关闭防火墙

```shell

systemctl stop firewalld
systemctl disable firewalld

```

* 禁用SELINUX

```
# 临时禁用
setenforce 0

# 永久禁用 
vim /etc/selinux/config    # 或者修改/etc/sysconfig/selinux
SELINUX=disabled
```

* 修改k8s.conf文件

```shell

cat <<EOF >  /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF
sysctl --system

```

* 关闭swap

```

# 临时关闭
swapoff -a
# 修改 /etc/fstab 文件，注释掉 SWAP 的自动挂载（永久关闭swap，重启后生效）
# 注释掉以下字段
/dev/mapper/cl-swap     swap                    swap    defaults        0 0
```

* 安装Docker
  * 卸载老版本

```
yum remove docker \
                  docker-client \
                  docker-client-latest \
                  docker-common \
                  docker-latest \
                  docker-latest-logrotate \
                  docker-logrotate \
                  docker-selinux \
                  docker-engine-selinux \
                  docker-engine
```
  * 使用yum安装

```

# step 1: 安装必要的一些系统工具
sudo yum install -y yum-utils device-mapper-persistent-data lvm2
# Step 2: 添加软件源信息
sudo yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
# Step 3: 更新并安装 Docker-CE
sudo yum makecache fast
sudo yum -y install docker-ce docker-ce-selinux
# 注意：
# 官方软件源默认启用了最新的软件，您可以通过编辑软件源的方式获取各个版本的软件包。例如官方并没有将测试版本的软件源置为可用，你可以通过以下方式开启。同理可以开启各种测试版本等。
# vim /etc/yum.repos.d/docker-ce.repo
#   将 [docker-ce-test] 下方的 enabled=0 修改为 enabled=1
#
# 安装指定版本的Docker-CE:
# Step 3.1: 查找Docker-CE的版本:
# yum list docker-ce.x86_64 --showduplicates | sort -r
#   Loading mirror speeds from cached hostfile
#   Loaded plugins: branch, fastestmirror, langpacks
#   docker-ce.x86_64            17.03.1.ce-1.el7.centos            docker-ce-stable
#   docker-ce.x86_64            17.03.1.ce-1.el7.centos            @docker-ce-stable
#   docker-ce.x86_64            17.03.0.ce-1.el7.centos            docker-ce-stable
#   Available Packages
# Step 3.2 : 安装指定版本的Docker-CE: (VERSION 例如上面的 17.03.0.ce.1-1.el7.centos)
sudo yum -y --setopt=obsoletes=0 install docker-ce-[VERSION] \
docker-ce-selinux-[VERSION]

# Step 4: 开启Docker服务
sudo systemctl enable docker && systemctl start docker

```
  * 错误信息

```

Error: Package: docker-ce-17.03.2.ce-1.el7.centos.x86_64 (docker-ce-stable)
           Requires: docker-ce-selinux >= 17.03.2.ce-1.el7.centos
           Available: docker-ce-selinux-17.03.0.ce-1.el7.centos.noarch (docker-ce-stable)
               docker-ce-selinux = 17.03.0.ce-1.el7.centos
           Available: docker-ce-selinux-17.03.1.ce-1.el7.centos.noarch (docker-ce-stable)
               docker-ce-selinux = 17.03.1.ce-1.el7.centos
           Available: docker-ce-selinux-17.03.2.ce-1.el7.centos.noarch (docker-ce-stable)
               docker-ce-selinux = 17.03.2.ce-1.el7.centos
 You could try using --skip-broken to work around the problem
 You could try running: rpm -Va --nofiles --nodigest

# 要先安装docker-ce-selinux-17.03.2.ce，否则安装docker-ce会报错
# 注意docker-ce-selinux的版本 要与docker的版本一直
yum install -y https://download.docker.com/linux/centos/7/x86_64/stable/Packages/docker-ce-selinux-17.03.3.ce-1.el7.noarch.rpm

# 或者
yum -y install https://mirrors.aliyun.com/docker-ce/linux/centos/7/x86_64/stable/Packages/docker-ce-selinux-17.03.3.ce-1.el7.noarch.rpm
```
  * 安装校验

```
docker version
Client:
 Version:      17.03.2-ce
 API version:  1.27
 Go version:   go1.7.5
 Git commit:   f5ec1e2
 Built:        Tue Jun 27 02:21:36 2017
 OS/Arch:      linux/amd64

Server:
 Version:      17.03.2-ce
 API version:  1.27 (minimum version 1.12)
 Go version:   go1.7.5
 Git commit:   f5ec1e2
 Built:        Tue Jun 27 02:21:36 2017
 OS/Arch:      linux/amd64
 Experimental: false
```
* 安装kubeadm/kubelet/kubectl
  * 修改yum安装源

```
cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF
```
  * 安装软件

```
yum install -y kubelet kubeadm kubectl
systemctl enable kubelet && systemctl start kubelet

 yum install -y kubelet-1.11.2-0 kubeadm-1.11.2-0 kubectl

```

* 初始化master节点
  * 提前准备所需镜像

```
K8S_VERSION=v1.11.2
ETCD_VERSTON=3.2.18
DASHBOARD_VERSION=v1.8.3
FLANNEL_VERSION=v0.10.0-amd64 
DNS_VERSION=1.1.3
PAUSE_VERSION=3.1
#基本组件

docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver-amd64:$K8S_VERSION

docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager-amd64:$K8S_VERSION

docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler-amd64:$K8S_VERSION

docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy-amd64:$K8S_VERSION

docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/etcd-amd64:$ETCD_VERSTON

docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/pause:$PAUSE_VERSION

docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/coredns:$DNS_VERSION

#网络组件

# docker pull quay.io/coreos/flannel:$FLANNEL_VERSION
docker pull registry.cn-hangzhou.aliyuncs.com/kubernetes_containers/flannel:$FLANNEL_VERSION

#修改 tag

docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver-amd64:$K8S_VERSION k8s.gcr.io/kube-apiserver-amd64:$K8S_VERSION

docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager-amd64:$K8S_VERSION k8s.gcr.io/kube-controller-manager-amd64:$K8S_VERSION

docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler-amd64:$K8S_VERSION k8s.gcr.io/kube-scheduler-amd64:$K8S_VERSION

docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy-amd64:$K8S_VERSION k8s.gcr.io/kube-proxy-amd64:$K8S_VERSION

docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/etcd-amd64:$ETCD_VERSTON k8s.gcr.io/etcd-amd64:$ETCD_VERSTON

docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/pause:$PAUSE_VERSION k8s.gcr.io/pause:$PAUSE_VERSION

docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/coredns:$DNS_VERSION k8s.gcr.io/coredns:$DNS_VERSION

docker tag registry.cn-hangzhou.aliyuncs.com/kubernetes_containers/flannel:$FLANNEL_VERSION quay.io/coreos/flannel:$FLANNEL_VERSION 
```

  * 配置kubeadm init初始化文件

<https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-init/?spm=a2c4e.11153940.blogcont626118.16.6ab3192cXToDYi#config-file>

```
apiVersion: kubeadm.k8s.io/v1alpha2
kind: MasterConfiguration
kubernetesVersion: v1.11.1    # kubernetes的版本
api:
  advertiseAddress: 172.16.242.129 # Master的IP地址
networking:
  podSubnet: 192.168.0.0/16    # pod网络的网段
imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers # ima
```

  * 运行初始化命令

```
kubeadm init --config kubeadm-init.yaml
```

  * 使kubectl正常工作
    * 多次运行kubeadm init命令，需要reset
    * kubelet不能启动成功原因之一

```
docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1 k8s.gcr.io/pause:3.1
```

```
kubeadm reset
```

### 8.1.3.搭建完整的Kubernetes集群

* 执行kubeadm reset

```

```

* 重新编写kubeadm启动文件

```
apiVersion: kubeadm.k8s.io/v1alpha2
kind: MasterConfiguration
kubernetesVersion: v1.11.2    # kubernetes的版本
api:
  advertiseAddress: 139.196.91.113 # Master的IP地址
networking:
  podSubnet: 192.168.0.0/16    # pod网络的网段
imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers # image的仓库源
controllerManagerExtraArgs:
  horizontal-pod-autoscaler-use-rest-clients: "true" # kube-controller-manager 能够使用自定义资源进行自动水平扩展。
  horizontal-pod-autoscaler-sync-period: "10s"
  node-monitor-grace-period: "10s"
apiServerExtraArgs:
  runtime-config: "api/all=true"
```

* 执行kubeadm init --config kubeadm-init.yaml

```
kubeadm init --config kubeadm-init.yaml
```

* 完成部署,记录join信息,这个 kubeadm join 命令，就是用来给这个 Master 节点添加更多工作节点（Worker）的命令。我们在后面部署 Worker 节点的时候马上会用到它，所以找一个地方把这条命令记录下来。

```
  kubeadm join 139.196.91.113:6443 --token 19cbhr.4hy2l3sgaks6ce2d --discovery-token-ca-cert-hash sha256:710ff4d0db5b70843b881a3fb910b8f8d8ab4dc39922afdfb0c3d1abb35222f2
```

* Kubeadm 还会提示我们第一次使用 Kubernetes 集群所需要的配置命令,而需要这些配置命令的原因是：Kubernetes 集群默认需要加密方式访问。所以，这几条命令，就是将刚刚部署生成的 Kubernetes 集群的安全配置文件，保存到当前用户的。Kube 目录下，kubectl 默认会使用这个目录下的授权信息访问 Kubernetes 集群。如果不这么做的话，我们每次都需要通过 export KUBECONFIG 环境变量告诉 kubectl 这个安全配置文件的位置。

```
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

```

* 查看节点状态,可以看到，这个 get 指令输出的结果里，Master 节点的状态是 NotReady
  
```
$ kubectl get nodes

NAME                   STATUS     ROLES    AGE   VERSION
ali-race-2c8g-node01   NotReady   master   8m    v1.11.2

```

* 查看节点详细信息,通过 kubectl describe 指令的输出，我们可以看到 NodeNotReady 的原因在于，我们尚未部署任何网络插件。

```
kubectl describe node ali-race-2c8g-node01

Name:               ali-race-2c8g-node01
Roles:              master
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/hostname=ali-race-2c8g-node01
                    node-role.kubernetes.io/master=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Tue, 02 Oct 2018 00:43:48 +0800
Taints:             node-role.kubernetes.io/master:NoSchedule
Unschedulable:      false
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  OutOfDisk        False   Tue, 02 Oct 2018 00:53:49 +0800   Tue, 02 Oct 2018 00:43:44 +0800   KubeletHasSufficientDisk     kubelet has sufficient disk space available
  MemoryPressure   False   Tue, 02 Oct 2018 00:53:49 +0800   Tue, 02 Oct 2018 00:43:44 +0800   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Tue, 02 Oct 2018 00:53:49 +0800   Tue, 02 Oct 2018 00:43:44 +0800   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Tue, 02 Oct 2018 00:53:49 +0800   Tue, 02 Oct 2018 00:43:44 +0800   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            False   Tue, 02 Oct 2018 00:53:49 +0800   Tue, 02 Oct 2018 00:43:44 +0800   KubeletNotReady              runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Addresses:
  InternalIP:  172.19.175.129
  Hostname:    ali-race-2c8g-node01
Capacity:
 cpu:                2
 ephemeral-storage:  41151808Ki
 hugepages-1Gi:      0
 hugepages-2Mi:      0
 memory:             8010688Ki
 pods:               110
Allocatable:
 cpu:                2
 ephemeral-storage:  37925506191
 hugepages-1Gi:      0
 hugepages-2Mi:      0
 memory:             7908288Ki
 pods:               110
System Info:
 Machine ID:                 963c2c41b08343f7b063dddac6b2e486
 System UUID:                D1A038C8-F360-4433-83ED-9925DDBB1059
 Boot ID:                    bbc71504-0cb7-46bc-b186-cfb65c904817
 Kernel Version:             3.10.0-514.26.2.el7.x86_64
 OS Image:                   CentOS Linux 7 (Core)
 Operating System:           linux
 Architecture:               amd64
 Container Runtime Version:  docker://17.3.3
 Kubelet Version:            v1.11.2
 Kube-Proxy Version:         v1.11.2
PodCIDR:                     192.168.0.0/24
Non-terminated Pods:         (5 in total)
  Namespace                  Name                                            CPU Requests  CPU Limits  Memory Requests  Memory Limits
  ---------                  ----                                            ------------  ----------  ---------------  -------------
  kube-system                etcd-ali-race-2c8g-node01                       0 (0%)        0 (0%)      0 (0%)           0 (0%)
  kube-system                kube-apiserver-ali-race-2c8g-node01             250m (12%)    0 (0%)      0 (0%)           0 (0%)
  kube-system                kube-controller-manager-ali-race-2c8g-node01    200m (10%)    0 (0%)      0 (0%)           0 (0%)
  kube-system                kube-proxy-k4vg9                                0 (0%)        0 (0%)      0 (0%)           0 (0%)
  kube-system                kube-scheduler-ali-race-2c8g-node01             100m (5%)     0 (0%)      0 (0%)           0 (0%)
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource  Requests    Limits
  --------  --------    ------
  cpu       550m (27%)  0 (0%)
  memory    0 (0%)      0 (0%)
Events:
  Type    Reason                   Age                From                              Message
  ----    ------                   ----               ----                              -------
  Normal  Starting                 10m                kubelet, ali-race-2c8g-node01     Starting kubelet.
  Normal  NodeAllocatableEnforced  10m                kubelet, ali-race-2c8g-node01     Updated Node Allocatable limit across pods
  Normal  NodeHasSufficientPID     10m (x5 over 10m)  kubelet, ali-race-2c8g-node01     Node ali-race-2c8g-node01 status is now: NodeHasSufficientPID
  Normal  NodeHasSufficientDisk    10m (x6 over 10m)  kubelet, ali-race-2c8g-node01     Node ali-race-2c8g-node01 status is now: NodeHasSufficientDisk
  Normal  NodeHasSufficientMemory  10m (x6 over 10m)  kubelet, ali-race-2c8g-node01     Node ali-race-2c8g-node01 status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    10m (x6 over 10m)  kubelet, ali-race-2c8g-node01     Node ali-race-2c8g-node01 status is now: NodeHasNoDiskPressure
  Normal  Starting                 9m43s              kube-proxy, ali-race-2c8g-node01  Starting kube-proxy.

```

* 还可以通过 kubectl 检查这个节点上各个系统 Pod 的状态，其中，kube- system 是 Kubernetes 项目预留的系统 Pod 的工作空间（Namepsace，注意它并不是 Linux Namespace，它只是 Kubernetes 划分不同工作空间的单位）,可以看到，CoreDNS、kube controller-manager 等依赖于网络的 Pod 都处于 Pending 状态，即调度失败。这当然是符合预期的：因为这个 Master 节点的网络尚未就绪。

```
kubectl get pods -n kube-system

NAME                                           READY   STATUS    RESTARTS   AGE
coredns-777d78ff6f-995dl                       0/1     Pending   0          14m
coredns-777d78ff6f-rcqzc                       0/1     Pending   0          14m
etcd-ali-race-2c8g-node01                      1/1     Running   0          14m
kube-apiserver-ali-race-2c8g-node01            1/1     Running   0          14m
kube-controller-manager-ali-race-2c8g-node01   1/1     Running   0          13m
kube-proxy-k4vg9                               1/1     Running   0          14m
kube-scheduler-ali-race-2c8g-node01            1/1     Running   0          14m

```

* 部署网络插件,在 Kubernetes 项目“一切皆容器”的设计理念指导下，部署网络插件非常简单，只需要执行一句 kubectl apply 指令，以 Weave 为例：

```
kubectl apply -f https://git.io/weave-kube-1.6
```

* 部署完成后，我们可以通过 kubectl get 重新检查 Pod 的状态,可以看到，所有的系统 Pod 都成功启动了，而刚刚部署的 Weave 网络插件则在 kube -system 下面新建了一个名叫 weave- net- cmk27 的 Pod，一般来说，这些 Pod 就是容器网络插件在每个节点上的控制组件。Kubernetes 支持容器网络插件，使用的是一个名叫 CNI 的通用接口，它也是当前容器网络的事实标准，市面上的所有容器网络开源项目都可以通过 CNI 接入 Kubernetes，比如 Flannel、Calico、Canal、Romana 等等，它我们的部署方式也都是类似的“一键部署”。关于这些开源项目的实现细节和差异，会在后续的网络部分详细介绍。至此，Kubernetes 的 Master 节点就部署完成了。如果你只需要一个单节点的 Kubernetes，现在你就可以使用了。不过，在默认情况下，Kubernetes 的 Master 节点是不能运行用户 Pod 的，所以还需要额外做一个小操作。

```
[root@ali-race-2C8G-node01 kubernetes]# kubectl get pods -n kube-system
NAME                                           READY   STATUS    RESTARTS   AGE
coredns-777d78ff6f-57d4c                       1/1     Running   0          4m
coredns-777d78ff6f-8gqkw                       1/1     Running   0          4m
etcd-ali-race-2c8g-node01                      1/1     Running   0          4m
kube-apiserver-ali-race-2c8g-node01            1/1     Running   0          4m
kube-controller-manager-ali-race-2c8g-node01   1/1     Running   0          3m
kube-proxy-ntlrk                               1/1     Running   0          4m
kube-scheduler-ali-race-2c8g-node01            1/1     Running   0          4m
weave-net-vv6gq                                2/2     Running   0          24s

```

* 部署Worker节点
  * Kubernetes 的 Worker 节点跟 Master 节点几乎是相同的，它我们运行着的都是一个 kubelet 组件。唯一的区别在于，在 kubeadm init 的过程中，kubelet 启动后，Master 节点上还会自动运行 kube-apiserver、kube-scheduler. Kube-controller-manger 这三个系统 Pod。所以，相比之下，部署 Worker 节点反而是最简单的，只需要两步即可完成。
  * 第一步，在所有 Worker 节点上执行“安装 kubeadm 和 Docker“一节的所有步骤。
  * 第二步，执行部署 Master 节点时生成的 kubeadm join 指令：

```
  kubeadm join 139.196.91.113:6443 --token 19cbhr.4hy2l3sgaks6ce2d --discovery-token-ca-cert-hash sha256:710ff4d0db5b70843b881a3fb910b8f8d8ab4dc39922afdfb0c3d1abb35222f2
```

* 通过Taint/Toleration调整Master执行Pod的策略,默认情况下 Master 节点是不允许运行用户 Pod 的。而 Kubernetes 做到这一 点，依靠的是Kubernetes的Taint/Toleration机制。它的原理非常简单：一旦某个节点被加上了一个 Taint，即被“打上了污点”，那么所有 Pod 就都不能在这个节点上运行，因为 Kubernetes 的 Pod 都有“洁癖”。除非，有个别的 Pod 声明自己能“容忍”这个“污点”，即声明了 Toleration，它才可以在这个节点上运行。其中，为节点打上“污点”（Taint）的命令是：

```
kubectl taint nodes node1 foo=bar:NoSchedule
```
这时，该 node1 节点上就会增加一个键值对格式的 Taint，即：foo=bar: NoSchedule。其中值里面的 NoSchedule，意味着这个 Taint 只会在调度新 Pod 时产生作用，而不会影响已经在 node1. 上运行的 Pod，哪怕它我们没有 Toleration。

那么 Pod 又如何声明 Toleration 呢？

* 只要在 Pod 的。Yaml 文件中的 spec 部分，加入 tolerations 字段即可：这个 Toleration 的含义是，这个 Pod 能“容忍”所有键值对为 foo=bar 的 Taint  (operator：“Equal”，“等于”操作）

```
apiVersion: v1
kind: Pod
...
spec:
  tolerations:
  - key: "foo"
    operator: "Equal"
    value: "bar"
    effect: "NoSchedule"

```

* 现在回到我们已经搭建的集群，上来。这时，如果你通过 kubectl describe 检查一下 Master 节点的 Taint 字段，就会有所发现了,可以看到，Master 节点默认被加上了 node-role. Kubernetes. Io/master: NoSchedule 这样一个“污点”，其中“键”是 node-role. Kubernetes. Io/master, 而没有提供“值”。

```
[root@ali-race-2C8G-node01 kubernetes]# kubectl describe node ali-race-2c8g-node01
Name:               ali-race-2c8g-node01
Roles:              master
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/hostname=ali-race-2c8g-node01
                    node-role.kubernetes.io/master=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Tue, 02 Oct 2018 01:07:13 +0800
Taints:             node-role.kubernetes.io/master:NoSchedule

```

* 此时，你就需要像下面这样用“Exists“操作符（operator：“Exists”，“存在”即可）来说明，该 Pod 能够容忍所有以 foo 为键的 Taint，才能让这个 Pod 运行在该 Master 节点上：

```
apiVersion: v1
kind: Pod
...
spec:
  tolerations:
  - key: "foo"
    operator: "Exists"
    effect: "NoSchedule"

```

* 当然，如果你就是想要一个单节点的 Kubernetes，删除这个 Taint 才是正确的选择,在“node- role. Kubernetes. Io/master”这个键后面加上了一个短横线“一”， 这个格式就意味着移除所有以“node-role. Kubernetes. Io /master”为键的 Taint。

```
kubectl taint nodes --all node-role.kubernetes.io/master-

```

* 部署Dashboard可视化插件

* 在 Kubernetes 社区中，有一个很受欢迎的 Dashboard 项目，它可以给用户提供一个可视化的 Web 界面来查看当前集群的各种信息。

```
访问,https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml

创建本地dashboard.yaml

设置yaml镜像地址为阿里云
    spec:
      containers:
      - name: kubernetes-dashboard
        image: registry.cn-hangzhou.aliyuncs.com/google_containers/kubernetes-dashboard-amd64:v1.10.0

kubectl apply -f dashboard.yaml
```

* 部署完成之后，我们就可以查看 Dashboard 对应的 Pod 的状态了：

```
$ kubectl get pods -n kube-system

kubernetes-dashboard-6948bdb78-f67xk   1/1       Running   0          1m

```

* 创建登录dashboard的账户,配置如下,并执行 kubectl apply -f k8s-admin.yaml

```
apiVersion: v1
kind: ServiceAccount
metadata:
  name: dashboard-admin
  namespace: kube-system
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata: 
  name: dashboard-admin
subjects:
  - kind: ServiceAccount
    name: dashboard-admin
    namespace: kube-system
roleRef:
  kind: ClusterRole
  name: cluster-admin
  apiGroup: rbac.authorization.k8s.io
```

* 获取token

```
kubectl apply -f k8s-admin.yaml
kubectl get secret -n kube-system
kubectl describe secret dashboard-admin-token-bwg7g -n kube-system
```

*

### 8.1.3.原理介绍

在理解了容器技术之后，你可能已经萌生出了这样一个想法，为什么不用容器部署 Kubernetes 呢？

这样，只要给每个Kubernetes组件做一个容器镜像，然后在每台宿主机上用dockerrun指令启动这些组件容器，部署不就完成了吗？事实上，在 Kubernetes 早期的部署脚本里，确实有一个脚本就是用 Docker 部署 Kubernetes 项目的，这个脚本相比于 SaltStack 等的部署方式，也的确简单了不少。

但是，这样做会带来一个很麻烦的问题，即：如何容器化kubelet。在上一篇文章中，已经提到 kubelet 是 Kubernetes 项目用来操作 Docker 等容器运行时的核心组件。可是，除了跟容器运行时打交道外，kubelet 在配置容器网络、管理容器数据卷时，都需要直接操作宿主机。而如果现在 kubelet 本身就运行在一个容器里，那么直接操作宿主机就会变得很麻烦。对于网络配置来说还好，kubelet 容器可以通过不开启 Network Namespace（即 Docker 的 host network 模式）的方式，直接共享宿主机的网络栈。

可是，要让 kubelet 隔着容器的 Mount Namespace 和文件系统，操作宿主机的文件系统，就有点儿困难了。比如，如果用户想要使用 NFS 做容器的持久化数据卷，那么 kubelet 就需要在容器进行绑定挂载前，在宿主机的指定目录上，先挂载 NFS 的远程目录。可是，这时候问题来了。由于现在 kubelet 是运行在容器里的，这就意味着它要做的这个“mount 一 F nfs”命令，被隔离在了一个单独的 Mount Namespace 中。即，kubelet 做的挂载操作，不能被“传播”到宿主机上。

对于这个问题，有人说，可以使用 setns（）系统调用，在宿主机的 Mount Namespace 中执行这些挂载操作；也有人说，应该让 Docker 支持一个-mnt=host 的参数。

但是，到目前为止，在容器里运行 kubelet，依然没有很好的解决办法，也不推荐你用容器去部署 Kubernetes 项目。

正因为如此，kubeadm 选择了一种妥协方案：把 kubelet 直接运行在宿主机上，然后使用容器部署其他的 Kubernetes 组件。所以，你使用 kubeadm 的第-一步，是在机器上手动安装 kubeadm、kubelet 和 kubectl 这三个二进制文件。当然，kubeadm 的作者已经为各个发行版的 Linux 准备好了安装包，所以你只需要执行：

```shell

$ yum install kubeadm

```

### 8.1.4.kubeadm init的工作流程

当执行 kubeadm init 指令后，kubeadm 首先要做的，是一系列的检査工作，以确定这台机器可以用来部署 Kubernetes。这一步检查，我们称为“Preflight Checks“，它可以为你省掉很多后续的麻烦。

其实，Preflight Checks 包括了很多方面，比如:

1. Linux 内核的版本必须是否是 3.10 以上？Linux Groups 模块是否可用？
2. 机器的 hostname 是否标准？在 Kubernetes 项目里，机器的名字以及一切存储在 Etcd 中的 AP이I 对象，都必须使用标准的 DNS 命名（RFC1123) 用户安装的 kubeadm 和 kubelet 的版本是否匹配？机器上是不是已经安装了 Kubernetes的二进制文件?
3. Kubernetes的工作端口10250/10251/10252 端口是不是已经被占用？ip、mount 等 Linux 指令是否存在？Docker 是否已经安装
4. 在通过了 Preflight Checks 之后，kubeadm 要为你做的，是生成 Kubernetes 对外提供服务所噐的各种证书和对应的目录

Kubernetes 对外提供服务时，除非专门开启“不安全模式”，否则都要通过 HTTPS 才能访问 kube- pserver。这就需要为 Kubernetes 集群配置好证书文件。

kubeadm 为 Kubernetes项目生成的证书文件都放在 Master节点的/etc/ kubernetes/pki 目录下。在这个目录下，最主要的证书文件是 ca. Ct 和对应的私钥 ca. Key。

此外，用户使用 kubectl 获取容器日志等 streaming 操作时，需要通过 kube- pserver 向 kubelet 发起请求，这个连接也必须是安全的。kubeadm 为这一步生成的是 pserver- kubelet client. Crt 文件，对应的私钥是 pserver- kubelet- client. Key

除此之外，Kubernetes 集群中还有 Aggregate Aplserver 等特性，也需要用到专门的证书，这里就不再一一列举了。需要指出的是，你可以选择不让 kubeadm 为你生成这些证书，而是拷贝现有的证书到如下证书的目录里:

```
/etc/kubernetes/pki/ca.{crt,key}
```

这时，kubeadm 就会跳过证书生成的步骤，把它完全交给用户处理证书生成后，kubeadm 接下来会为其他组件生成访问 kube- pserver 所需的配置文件。这些文件的路径是：/etc/ kubernetes/xxx.conf

```
ls /etc/kubernetes/
admin.conf  controller-manager.conf  kubelet.conf  scheduler.conf
```

这些文件里面记录的是，当前这个 Master 节点的服务器地址、监听端口、证书目录等信息。这样，对应的客户端（比如 scheduler, kubelet 等），可以直接加载相应的文件，使用里面的信息与 kube- pserver 建立安全连接

接下来，kubeadm 会为 Master 组件生成 Pod 配置文件。已经在上一篇文章中和你介绍过 Kubernetes 有三个 Master 组件 kube- pserver、kube- controller- manager、kube scheduler，而它我们都会被使用 Pod 的方式部署起来。

你可能会有些疑问：这时，Kubernetes 集群尚不存在，难道 kubeadm 会直接执行 docker run 来启动这些容器吗?当然不是。

在 Kubernetes 中，有一种特殊的容器启动方法叫做“Static Pod”。它允许你把要部署的 Pod 的 YAML 文件放在一个指定的目录里。这样，当这台机器上的 kubelet 启动时，它会自动检查这个目录，加载所有的 Pod YAML 文件，然后在这台机器上启动它我们。

从这一点也可以看出，kubelet 在 Kubernetes 项目中的地位非常高，在设计上它就是一个完全独立的组件，而其他 Master 组件，则更像是辅助性的系统容器。

在 kubeadm中, Master 组件的YAML 文件会被生成在/etc/ kubernetes/ manifests 路径下。比 3 n, kube-apiserver yaml

```
apiVersion: v1
kind: Pod
metadata:
  annotations:
    scheduler.alpha.kubernetes.io/critical-pod: ""
  creationTimestamp: null
  labels:
    component: kube-apiserver
    tier: control-plane
  name: kube-apiserver
  namespace: kube-system
spec:
  containers:
  - command:
    - kube-apiserver
    - --authorization-mode=Node,RBAC
    - --runtime-config=api/all=true
    - --advertise-address=10.168.0.2
    ...
    - --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
    - --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
    image: k8s.gcr.io/kube-apiserver-amd64:v1.11.1
    imagePullPolicy: IfNotPresent
    livenessProbe:
      ...
    name: kube-apiserver
    resources:
      requests:
        cpu: 250m
    volumeMounts:
    - mountPath: /usr/share/ca-certificates
      name: usr-share-ca-certificates
      readOnly: true
    ...
  hostNetwork: true
  priorityClassName: system-cluster-critical
  volumes:
  - hostPath:
      path: /etc/ca-certificates
      type: DirectoryOrCreate
    name: etc-ca-certificates
  ...
```

关于ー个 Pod 的 YAML 文件怎么写、里面的字段如何解读，会在后续专门的文章中为你详细分析。在这里，你只需要关注这样几个信息:

1. 这个 Pod 里只定义了ー个容器，它使用的镜像是: k8s.gcr,io/kube- pserver amd64: v1.11.1。这个镜像是 Kubernetes 官方维护的一个组件镜像。
2. 这个容器的启动命令（commands）是 kube- pserver-- authorization-mode-Node, RBAC这样一句非常长的命令。其实，它就是容器里 kube- pserver 这个二进制文件再加上指定的配置参数而已
3. 如果你要修改一个已有集群的 kube- pserver 的配置，需要修改这个 YAML 文件
4. 这些组件的参数也可以在部署时指定，很快就会讲解到

在这一步完成后，kubeadm 还会再生成一个 Etcd 的 Pod YAML 文件，用来通过同样的 Static Pod 的方式启动 Etcd。所以，最后 Master 组件的 Pod YAML 文件如下所示:

```
$ ls /etc/kubernetes/manifests/
etcd.yaml  kube-apiserver.yaml  kube-controller-manager.yaml  kube-scheduler.yaml
```

而一旦这些 YAML 文件出现在被 kubelet监视的/etc/ kubernetes/ manifests 目录下，kubelet 就会自动创建这些 YAML 文件中定义的 Pod，即 Master 组件的容器。

Master 容器启动后，kubeadm 会通过检查 localhost:6443/ health 这个 Master 组件的健康检查 URL，等待 Master 组件完全运行起来

然后，kubeadm 就会为集群生成一个 bootstrap token。在后面，只要持有这个 token，任何一个安装了 kubelet 和 kuban 的节点，都可以通过 kubeadm join 加入到这个集群当中

这个 token 的值和使用方法会，会在 kubeadm init 结束后被打印出来。

在 token 生成之后，kubeadm 会将 ca. Crt 等 Master 节点的重要信息，通过 Configmap 的方式保存在 Etcd 当中，供后续部署 Node 节点使用。这个 Confgmap 的名字是 cluster-info。

kubeadm init 的最后一步，就是安装默认插件。Kubernetes 默认 kube- proxy 和 DNS 这两个插件是必须安装的。它我们分别用来提供整个集群的服务发现和 DNS 功能。其实，这两个插件也只是两个容器镜像而已，所以 kubeadm 只要用 Kubernetes 客户端创建两个 Pod 就可以了.


### 8.1.5.kubeadm join的工作流程

这个流程其实非常简单，kubeadm init 生成 bootstrap token 之后，你就可以在任意一台安装了 kubelet 和 kubeadm 的机器 H 执行 kubeadm join 了。

可是，为什么执行 kubeadm join 需要这样一个 token 呢？

因为，任何一台机器想要成为 Kubernetes 集群中的一个节点，就必须在集群的 kube apiserver 上注册。可是，要想跟 apiserver 打交道，这台机器就必须要获取到相应的证书文件（CA 文件）。可是，为了能够一键安装，我们就不能让用户去 Master 节点上手动拷贝这些文件。

所以，kubeadm 至少需要发起一次“不安全模式”的访问到 kube -apiserver，从而拿到保存在 ConfigMap 中的 cluster-info（它保存了 APIServer 的授权信息）。而 bootstrap token，扮演的就是这个过程中的安全验证的角色。

只要有了 cluster-info 里的 kube- apiserver 的地址、端口、证书，kubelet 就可以以“安全模式”连接到 apiserver 上，这样一个新的节点就部署完成了。

接下来，你只要在其他节点上重复这个指令就可以了。


### 8.1.6.配置kubeadm的部署参数

在前面讲解了 kubeadm 部署 Kubernetes 集群最关键的两个步骤，kubeadm init 和 kubeadm join。相信你一定会有这样的疑问：kubeadm 确实简单易用，可是又该如何定制的集群组件参数呢？

比如，要指定 kube-apiserver 的启动参数，该怎么办？

推荐使用 kubeadm init 部署 Master 节点时，使用下面这条指令：

```
$ kubeadm init --config kubeadm.yaml
```

给 kubeadm 提供一个 YAML 文件:

```
apiVersion: kubeadm.k8s.io/v1alpha2
kind: MasterConfiguration
kubernetesVersion: v1.11.0
api:
  advertiseAddress: 192.168.0.102
  bindPort: 6443
  ...
etcd:
  local:
    dataDir: /var/lib/etcd
    image: ""
imageRepository: k8s.gcr.io
kubeProxy:
  config:
    bindAddress: 0.0.0.0
    ...
kubeletConfiguration:
  baseConfig:
    address: 0.0.0.0
    ...
networking:
  dnsDomain: cluster.local
  podSubnet: ""
  serviceSubnet: 10.96.0.0/12
nodeRegistration:
  criSocket: /var/run/dockershim.sock
  ...

```

通过制定这样一个部署参数配置文件，你就可以很方便地在这个文件里填写各种自定义的部署参数了。比如，现在要指定 kube- apiserver 的参数，那么只要在这个文件里加上这样一段信息：

```
...
apiServerExtraArgs:
  advertise-address: 192.168.0.103
  anonymous-auth: false
  enable-admission-plugins: AlwaysPullImages,DefaultStorageClass
  audit-log-path: /home/johndoe/audit.log

```

然后，kubeadm 就会使用上面这些信息替换/etc/kubernetes/manifests/kube apiserver. Yaml 里的 command 字段里的参数了。

而这个 YAML 文件提供的可配置项远不止这些。比如，你还可以修改 kubelet 和 kube- proxy 的配置，修改 Kubernetes 使用的基础镜像的 URL（默认的 k8s. Gcr. io/xxx镜像URL 在国内访问是有困难的），指定自己的证书文件，指定特殊的容器运行时等等。这些配置项，就留给你在后续实践中探索了。