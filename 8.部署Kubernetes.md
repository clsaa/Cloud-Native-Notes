# 8.部署Kubernetes

## 8.1.Kubernetes部署利器kubeadm


### 8.1.1.基本介绍

kubeadm的目的是让用户能够使用如下两条命令来简单的部署kubernetes集群

```
# 创建一个 Master 节点
$ kubeadm init

# 将一个 Node 节点加入到当前集群中
$ kubeadm join <Master 节点的 IP 和端口 >

```

### 8.1.2.环境准备

原文<https://yq.aliyun.com/articles/626118>

* 设置主机名

```shell

hostnamectl set-hostname ali-race-2C8G-node01

hostnamectl set-hostname ali-race-2C8G-node02 
```

* 设置IP映射

```
vim /etc/hosts
#添加如下IP与主机名映射关系
172.19.175.129 ali-race-2C8G-node01
172.19.175.130 ali-race-2C8G-node02
```

* 关闭防火墙

```shell

systemctl stop firewalld
systemctl disable firewalld

```

* 禁用SELINUX

```shell

# 永久禁用 
vim /etc/selinux/config
SELINUX=disabled
```

* 修改k8s.conf文件

```shell

cat <<EOF >  /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF

sysctl --system

```

* 关闭swap

```
# 临时关闭
swapoff -a
# 修改 /etc/fstab 文件，注释掉 SWAP 的自动挂载（永久关闭swap，重启后生效）
vim /etc/fstab
# 注释掉以下字段
/dev/mapper/cl-swap     swap                    swap    defaults        0 0
```

### 8.1.3.安装Docker

* 安装Docker
  * 卸载老版本Docker

```
yum remove docker \
                  docker-client \
                  docker-client-latest \
                  docker-common \
                  docker-latest \
                  docker-latest-logrotate \
                  docker-logrotate \
                  docker-selinux \
                  docker-engine-selinux \
                  docker-engine
```
  * 使用yum安装Docker

```

# step 1: 安装必要的一些系统工具
sudo yum install -y yum-utils device-mapper-persistent-data lvm2
# Step 2: 添加软件源信息
sudo yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
# Step 3: 更新并安装 Docker-CE
sudo yum makecache fast
# 注意：
# 官方软件源默认启用了最新的软件，您可以通过编辑软件源的方式获取各个版本的软件包。例如官方并没有将测试版本的软件源置为可用，你可以通过以下方式开启。同理可以开启各种测试版本等。
# vim /etc/yum.repos.d/docker-ce.repo
#   将 [docker-ce-test] 下方的 enabled=0 修改为 enabled=1
#
# 安装指定版本的Docker-CE:
# Step 3.1: 查找Docker-CE的版本:
# yum list docker-ce.x86_64 --showduplicates | sort -r
#   Loading mirror speeds from cached hostfile
#   Loaded plugins: branch, fastestmirror, langpacks
#   docker-ce.x86_64            17.03.1.ce-1.el7.centos            docker-ce-stable
#   docker-ce.x86_64            17.03.1.ce-1.el7.centos            @docker-ce-stable
#   docker-ce.x86_64            17.03.0.ce-1.el7.centos            docker-ce-stable
#   Available Packages
# Step 3.2 : 安装指定版本的Docker-CE: (VERSION 例如上面的 17.03.0.ce.1-1.el7.centos)
sudo yum -y --setopt=obsoletes=0 install docker-ce-[VERSION] \
docker-ce-selinux-[VERSION]

# 如

sudo yum -y --setopt=obsoletes=0 install docker-ce-18.03.1.ce-1.el7.centos \
docker-ce-selinux-18.03.1.ce-1.el7.centos

# Step 4: 开启Docker服务
sudo systemctl enable docker && systemctl start docker

```
  * 可能出现的错误信息

```

Error: Package: docker-ce-17.03.2.ce-1.el7.centos.x86_64 (docker-ce-stable)
           Requires: docker-ce-selinux >= 17.03.2.ce-1.el7.centos
           Available: docker-ce-selinux-17.03.0.ce-1.el7.centos.noarch (docker-ce-stable)
               docker-ce-selinux = 17.03.0.ce-1.el7.centos
           Available: docker-ce-selinux-17.03.1.ce-1.el7.centos.noarch (docker-ce-stable)
               docker-ce-selinux = 17.03.1.ce-1.el7.centos
           Available: docker-ce-selinux-17.03.2.ce-1.el7.centos.noarch (docker-ce-stable)
               docker-ce-selinux = 17.03.2.ce-1.el7.centos
 You could try using --skip-broken to work around the problem
 You could try running: rpm -Va --nofiles --nodigest

# 要先安装docker-ce-selinux-17.03.2.ce，否则安装docker-ce会报错
# 注意docker-ce-selinux的版本 要与docker的版本一直
yum install -y https://download.docker.com/linux/centos/7/x86_64/stable/Packages/docker-ce-selinux-17.03.3.ce-1.el7.noarch.rpm

# 或者
yum -y install https://mirrors.aliyun.com/docker-ce/linux/centos/7/x86_64/stable/Packages/docker-ce-selinux-17.03.3.ce-1.el7.noarch.rpm
```
  * 安装校验

```
docker version
Client:
 Version:      17.03.2-ce
 API version:  1.27
 Go version:   go1.7.5
 Git commit:   f5ec1e2
 Built:        Tue Jun 27 02:21:36 2017
 OS/Arch:      linux/amd64

Server:
 Version:      17.03.2-ce
 API version:  1.27 (minimum version 1.12)
 Go version:   go1.7.5
 Git commit:   f5ec1e2
 Built:        Tue Jun 27 02:21:36 2017
 OS/Arch:      linux/amd64
 Experimental: false
```

* 配置阿里云Docker镜像加速器

```
sudo mkdir -p /etc/docker
sudo tee /etc/docker/daemon.json <<-'EOF'
{
  "registry-mirrors": ["https://uwekamch.mirror.aliyuncs.com"]
}
EOF
sudo systemctl daemon-reload
sudo systemctl restart docker
```

### 8.1.4.安装K8S

* 安装kubeadm/kubelet/kubectl
  * 修改yum安装源

```
cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF
```
  * 安装软件

```
# yum install -y kubelet-[VERSION] kubeadm-[VERSION] kubectl-[VERSION]

yum install -y kubelet-1.13.1-0 kubeadm-1.13.1-0 kubectl-1.13.1-0

systemctl enable kubelet && systemctl start kubelet

```

* 配置 kubelet

安装完成后，我们还需要对 kubelet 进行配置，因为用 yum 源的方式安装的 kubelet 生成的配置文件将参数 --cgroup-driver 改成了 systemd，而 docker 的 cgroup-driver 是 cgroupfs，这二者必须一致才行，我们可以通过 docker info 命令查看：

```
$ docker info | grep Cgroup
Cgroup Driver: cgroupfs

```

修改文件 kubelet 的配置文件 /etc/systemd/system/kubelet.service.d/10-kubeadm.conf ，将其中的 KUBELET_CGROUP_ARGS 参数更改成 cgroupfs：

```
Environment="KUBELET_CGROUP_ARGS=--cgroup-driver=cgroupfs"
```

* 初始化master节点
  * 提前准备所需镜像

```
#与kubelet版本一直
K8S_VERSION=v1.13.1
ETCD_VERSTON=3.2.24
DASHBOARD_VERSION=v1.8.3
FLANNEL_VERSION=v0.10.0-amd64 
DNS_VERSION=1.2.6
PAUSE_VERSION=3.1
#基本组件

docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver-amd64:$K8S_VERSION

docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager-amd64:$K8S_VERSION

docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler-amd64:$K8S_VERSION

docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy-amd64:$K8S_VERSION

docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/etcd-amd64:$ETCD_VERSTON

docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/pause:$PAUSE_VERSION

docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/coredns:$DNS_VERSION

#网络组件

# docker pull quay.io/coreos/flannel:$FLANNEL_VERSION
docker pull registry.cn-hangzhou.aliyuncs.com/kubernetes_containers/flannel:$FLANNEL_VERSION

#修改 tag

docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver-amd64:$K8S_VERSION k8s.gcr.io/kube-apiserver-amd64:$K8S_VERSION

docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager-amd64:$K8S_VERSION k8s.gcr.io/kube-controller-manager-amd64:$K8S_VERSION

docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler-amd64:$K8S_VERSION k8s.gcr.io/kube-scheduler-amd64:$K8S_VERSION

docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy-amd64:$K8S_VERSION k8s.gcr.io/kube-proxy-amd64:$K8S_VERSION

docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/etcd-amd64:$ETCD_VERSTON k8s.gcr.io/etcd-amd64:$ETCD_VERSTON

docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/pause:$PAUSE_VERSION k8s.gcr.io/pause:$PAUSE_VERSION

docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/coredns:$DNS_VERSION k8s.gcr.io/coredns:$DNS_VERSION

docker tag registry.cn-hangzhou.aliyuncs.com/kubernetes_containers/flannel:$FLANNEL_VERSION quay.io/coreos/flannel:$FLANNEL_VERSION 
```


### 8.1.3.搭建完整的Kubernetes集群

* 执行kubeadm reset

```
kubeadm reset
```

* 重新编写kubeadm启动文件

```
apiVersion: kubeadm.k8s.io/v1beta1
kind: ClusterConfiguration
kubernetesVersion: v1.13.1    # kubernetes的版本
api:
  advertiseAddress: 139.196.91.113 # Master的IP地址
networking:
  serviceSubnet: 10.96.0.0/12 # service网络的网段
  podSubnet: 10.68.0.0/16    # pod网络的网段
imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers # image的仓库源
controllerManagerExtraArgs:
  horizontal-pod-autoscaler-use-rest-clients: "true" # kube-controller-manager 能够使用自定义资源进行自动水平扩展。
  horizontal-pod-autoscaler-sync-period: "10s"
  node-monitor-grace-period: "10s"
apiServerExtraArgs:
  runtime-config: "api/all=true"
```

* 执行kubeadm init --config kubeadm-init.yaml

```
kubeadm init --config kubeadm-init.yaml
```

* 完成部署,记录join信息,这个 kubeadm join 命令，就是用来给这个 Master 节点添加更多工作节点（Worker）的命令。我们在后面部署 Worker 节点的时候马上会用到它，所以找一个地方把这条命令记录下来。

```
  kubeadm join 139.196.91.113:6443 --token 19cbhr.4hy2l3sgaks6ce2d --discovery-token-ca-cert-hash sha256:710ff4d0db5b70843b881a3fb910b8f8d8ab4dc39922afdfb0c3d1abb35222f2
```

* Kubeadm 还会提示我们第一次使用 Kubernetes 集群所需要的配置命令,而需要这些配置命令的原因是：Kubernetes 集群默认需要加密方式访问。所以，这几条命令，就是将刚刚部署生成的 Kubernetes 集群的安全配置文件，保存到当前用户的.kube 目录下，kubectl 默认会使用这个目录下的授权信息访问 Kubernetes 集群。如果不这么做的话，我们每次都需要通过 export KUBECONFIG 环境变量告诉 kubectl 这个安全配置文件的位置。

```
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

```

* 查看节点状态,可以看到，这个 get 指令输出的结果里，Master 节点的状态是 NotReady
  
```
$ kubectl get nodes

NAME                   STATUS     ROLES    AGE   VERSION
ali-race-2c8g-node01   NotReady   master   8m    v1.11.2

```

* 查看节点详细信息,通过 kubectl describe 指令的输出，我们可以看到 NodeNotReady 的原因在于，我们尚未部署任何网络插件。

```
kubectl describe node ali-race-2c8g-node01

Name:               ali-race-2c8g-node01
Roles:              master
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/hostname=ali-race-2c8g-node01
                    node-role.kubernetes.io/master=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Tue, 02 Oct 2018 00:43:48 +0800
Taints:             node-role.kubernetes.io/master:NoSchedule
Unschedulable:      false
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  OutOfDisk        False   Tue, 02 Oct 2018 00:53:49 +0800   Tue, 02 Oct 2018 00:43:44 +0800   KubeletHasSufficientDisk     kubelet has sufficient disk space available
  MemoryPressure   False   Tue, 02 Oct 2018 00:53:49 +0800   Tue, 02 Oct 2018 00:43:44 +0800   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Tue, 02 Oct 2018 00:53:49 +0800   Tue, 02 Oct 2018 00:43:44 +0800   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Tue, 02 Oct 2018 00:53:49 +0800   Tue, 02 Oct 2018 00:43:44 +0800   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            False   Tue, 02 Oct 2018 00:53:49 +0800   Tue, 02 Oct 2018 00:43:44 +0800   KubeletNotReady              runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Addresses:
  InternalIP:  172.19.175.129
  Hostname:    ali-race-2c8g-node01
Capacity:
 cpu:                2
 ephemeral-storage:  41151808Ki
 hugepages-1Gi:      0
 hugepages-2Mi:      0
 memory:             8010688Ki
 pods:               110
Allocatable:
 cpu:                2
 ephemeral-storage:  37925506191
 hugepages-1Gi:      0
 hugepages-2Mi:      0
 memory:             7908288Ki
 pods:               110
System Info:
 Machine ID:                 963c2c41b08343f7b063dddac6b2e486
 System UUID:                D1A038C8-F360-4433-83ED-9925DDBB1059
 Boot ID:                    bbc71504-0cb7-46bc-b186-cfb65c904817
 Kernel Version:             3.10.0-514.26.2.el7.x86_64
 OS Image:                   CentOS Linux 7 (Core)
 Operating System:           linux
 Architecture:               amd64
 Container Runtime Version:  docker://17.3.3
 Kubelet Version:            v1.11.2
 Kube-Proxy Version:         v1.11.2
PodCIDR:                     192.168.0.0/24
Non-terminated Pods:         (5 in total)
  Namespace                  Name                                            CPU Requests  CPU Limits  Memory Requests  Memory Limits
  ---------                  ----                                            ------------  ----------  ---------------  -------------
  kube-system                etcd-ali-race-2c8g-node01                       0 (0%)        0 (0%)      0 (0%)           0 (0%)
  kube-system                kube-apiserver-ali-race-2c8g-node01             250m (12%)    0 (0%)      0 (0%)           0 (0%)
  kube-system                kube-controller-manager-ali-race-2c8g-node01    200m (10%)    0 (0%)      0 (0%)           0 (0%)
  kube-system                kube-proxy-k4vg9                                0 (0%)        0 (0%)      0 (0%)           0 (0%)
  kube-system                kube-scheduler-ali-race-2c8g-node01             100m (5%)     0 (0%)      0 (0%)           0 (0%)
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource  Requests    Limits
  --------  --------    ------
  cpu       550m (27%)  0 (0%)
  memory    0 (0%)      0 (0%)
Events:
  Type    Reason                   Age                From                              Message
  ----    ------                   ----               ----                              -------
  Normal  Starting                 10m                kubelet, ali-race-2c8g-node01     Starting kubelet.
  Normal  NodeAllocatableEnforced  10m                kubelet, ali-race-2c8g-node01     Updated Node Allocatable limit across pods
  Normal  NodeHasSufficientPID     10m (x5 over 10m)  kubelet, ali-race-2c8g-node01     Node ali-race-2c8g-node01 status is now: NodeHasSufficientPID
  Normal  NodeHasSufficientDisk    10m (x6 over 10m)  kubelet, ali-race-2c8g-node01     Node ali-race-2c8g-node01 status is now: NodeHasSufficientDisk
  Normal  NodeHasSufficientMemory  10m (x6 over 10m)  kubelet, ali-race-2c8g-node01     Node ali-race-2c8g-node01 status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    10m (x6 over 10m)  kubelet, ali-race-2c8g-node01     Node ali-race-2c8g-node01 status is now: NodeHasNoDiskPressure
  Normal  Starting                 9m43s              kube-proxy, ali-race-2c8g-node01  Starting kube-proxy.

```

* 还可以通过 kubectl 检查这个节点上各个系统 Pod 的状态，其中，kube- system 是 Kubernetes 项目预留的系统 Pod 的工作空间（Namepsace，注意它并不是 Linux Namespace，它只是 Kubernetes 划分不同工作空间的单位）,可以看到，CoreDNS、kube controller-manager 等依赖于网络的 Pod 都处于 Pending 状态，即调度失败。这当然是符合预期的：因为这个 Master 节点的网络尚未就绪。

```
kubectl get pods -n kube-system

NAME                                           READY   STATUS    RESTARTS   AGE
coredns-777d78ff6f-995dl                       0/1     Pending   0          14m
coredns-777d78ff6f-rcqzc                       0/1     Pending   0          14m
etcd-ali-race-2c8g-node01                      1/1     Running   0          14m
kube-apiserver-ali-race-2c8g-node01            1/1     Running   0          14m
kube-controller-manager-ali-race-2c8g-node01   1/1     Running   0          13m
kube-proxy-k4vg9                               1/1     Running   0          14m
kube-scheduler-ali-race-2c8g-node01            1/1     Running   0          14m

```

* 部署网络插件,在 Kubernetes 项目“一切皆容器”的设计理念指导下，部署网络插件非常简单，只需要执行一句 kubectl apply 指令，以 Weave 为例：

```
kubectl apply -f https://git.io/weave-kube-1.6
```

* 部署完成后，我们可以通过 kubectl get 重新检查 Pod 的状态,可以看到，所有的系统 Pod 都成功启动了，而刚刚部署的 Weave 网络插件则在 kube -system 下面新建了一个名叫 weave- net- cmk27 的 Pod，一般来说，这些 Pod 就是容器网络插件在每个节点上的控制组件。Kubernetes 支持容器网络插件，使用的是一个名叫 CNI 的通用接口，它也是当前容器网络的事实标准，市面上的所有容器网络开源项目都可以通过 CNI 接入 Kubernetes，比如 Flannel、Calico、Canal、Romana 等等，它我们的部署方式也都是类似的“一键部署”。关于这些开源项目的实现细节和差异，会在后续的网络部分详细介绍。至此，Kubernetes 的 Master 节点就部署完成了。如果你只需要一个单节点的 Kubernetes，现在你就可以使用了。不过，在默认情况下，Kubernetes 的 Master 节点是不能运行用户 Pod 的，所以还需要额外做一个小操作。

```
[root@ali-race-2C8G-node01 kubernetes]# kubectl get pods -n kube-system
NAME                                           READY   STATUS    RESTARTS   AGE
coredns-777d78ff6f-57d4c                       1/1     Running   0          4m
coredns-777d78ff6f-8gqkw                       1/1     Running   0          4m
etcd-ali-race-2c8g-node01                      1/1     Running   0          4m
kube-apiserver-ali-race-2c8g-node01            1/1     Running   0          4m
kube-controller-manager-ali-race-2c8g-node01   1/1     Running   0          3m
kube-proxy-ntlrk                               1/1     Running   0          4m
kube-scheduler-ali-race-2c8g-node01            1/1     Running   0          4m
weave-net-vv6gq                                2/2     Running   0          24s

```

* 部署Worker节点
  * Kubernetes 的 Worker 节点跟 Master 节点几乎是相同的，它我们运行着的都是一个 kubelet 组件。唯一的区别在于，在 kubeadm init 的过程中，kubelet 启动后，Master 节点上还会自动运行 kube-apiserver、kube-scheduler. Kube-controller-manger 这三个系统 Pod。所以，相比之下，部署 Worker 节点反而是最简单的，只需要两步即可完成。
  * 第一步，在所有 Worker 节点上执行“安装 kubeadm 和 Docker“一节的所有步骤。
  * 第二步，执行部署 Master 节点时生成的 kubeadm join 指令：

```
  kubeadm join 139.196.91.113:6443 --token 19cbhr.4hy2l3sgaks6ce2d --discovery-token-ca-cert-hash sha256:710ff4d0db5b70843b881a3fb910b8f8d8ab4dc39922afdfb0c3d1abb35222f2
```

* 通过Taint/Toleration调整Master执行Pod的策略,默认情况下 Master 节点是不允许运行用户 Pod 的。而 Kubernetes 做到这一 点，依靠的是Kubernetes的Taint/Toleration机制。它的原理非常简单：一旦某个节点被加上了一个 Taint，即被“打上了污点”，那么所有 Pod 就都不能在这个节点上运行，因为 Kubernetes 的 Pod 都有“洁癖”。除非，有个别的 Pod 声明自己能“容忍”这个“污点”，即声明了 Toleration，它才可以在这个节点上运行。其中，为节点打上“污点”（Taint）的命令是：

```
kubectl taint nodes node1 foo=bar:NoSchedule
```
这时，该 node1 节点上就会增加一个键值对格式的 Taint，即：foo=bar: NoSchedule。其中值里面的 NoSchedule，意味着这个 Taint 只会在调度新 Pod 时产生作用，而不会影响已经在 node1. 上运行的 Pod，哪怕它我们没有 Toleration。

那么 Pod 又如何声明 Toleration 呢？

* 只要在 Pod 的。Yaml 文件中的 spec 部分，加入 tolerations 字段即可：这个 Toleration 的含义是，这个 Pod 能“容忍”所有键值对为 foo=bar 的 Taint  (operator：“Equal”，“等于”操作）

```
apiVersion: v1
kind: Pod
...
spec:
  tolerations:
  - key: "foo"
    operator: "Equal"
    value: "bar"
    effect: "NoSchedule"

```

* 现在回到我们已经搭建的集群，上来。这时，如果你通过 kubectl describe 检查一下 Master 节点的 Taint 字段，就会有所发现了,可以看到，Master 节点默认被加上了 node-role. Kubernetes. Io/master: NoSchedule 这样一个“污点”，其中“键”是 node-role. Kubernetes. Io/master, 而没有提供“值”。

```
[root@ali-race-2C8G-node01 kubernetes]# kubectl describe node ali-race-2c8g-node01
Name:               ali-race-2c8g-node01
Roles:              master
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/hostname=ali-race-2c8g-node01
                    node-role.kubernetes.io/master=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Tue, 02 Oct 2018 01:07:13 +0800
Taints:             node-role.kubernetes.io/master:NoSchedule

```

* 此时，你就需要像下面这样用“Exists“操作符（operator：“Exists”，“存在”即可）来说明，该 Pod 能够容忍所有以 foo 为键的 Taint，才能让这个 Pod 运行在该 Master 节点上：

```
apiVersion: v1
kind: Pod
...
spec:
  tolerations:
  - key: "foo"
    operator: "Exists"
    effect: "NoSchedule"

```

* 当然，如果你就是想要一个单节点的 Kubernetes，删除这个 Taint 才是正确的选择,在“node- role. Kubernetes. Io/master”这个键后面加上了一个短横线“一”， 这个格式就意味着移除所有以“node-role. Kubernetes. Io /master”为键的 Taint。

```
kubectl taint nodes --all node-role.kubernetes.io/master-

```

* 部署Dashboard可视化插件

* 在 Kubernetes 社区中，有一个很受欢迎的 Dashboard 项目，它可以给用户提供一个可视化的 Web 界面来查看当前集群的各种信息。

```
访问,https://raw.githubusercontent.com/kubernetes/dashboard/v1.10.1/src/deploy/recommended/kubernetes-dashboard.yaml

创建本地dashboard.yaml

设置yaml镜像地址为阿里云
    spec:
      containers:
      - name: kubernetes-dashboard
        image: registry.cn-hangzhou.aliyuncs.com/google_containers/kubernetes-dashboard-amd64:v1.10.0

kubectl apply -f dashboard.yaml
```

* 部署完成之后，我们就可以查看 Dashboard 对应的 Pod 的状态了：

```
$ kubectl get pods -n kube-system

kubernetes-dashboard-6948bdb78-f67xk   1/1       Running   0          1m

```

* 创建登录dashboard的账户,配置如下,并执行 kubectl apply -f k8s-admin.yaml

```
apiVersion: v1
kind: ServiceAccount
metadata:
  name: dashboard-admin
  namespace: kube-system
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata: 
  name: dashboard-admin
subjects:
  - kind: ServiceAccount
    name: dashboard-admin
    namespace: kube-system
roleRef:
  kind: ClusterRole
  name: cluster-admin
  apiGroup: rbac.authorization.k8s.io
```

* 获取token

```
kubectl apply -f k8s-admin.yaml
kubectl get secret -n kube-system
kubectl describe secret dashboard-admin-token-bwg7g -n kube-system
```

### 8.1.4.服务暴露到公网

* kubernetes中的Service暴露到外部有三种方式，分别是：
  * LoadBlancer Service
  * NodePort Service
  * Ingress

LoadBlancer Service是kubernetes深度结合云平台的一个组件；当使用LoadBlancer Service暴露服务时，实际上是通过向底层云平台申请创建一个负载均衡器来向外暴露服务；目前LoadBlancer Service支持的云平台已经相对完善，比如国外的GCE、DigitalOcean，国内的 阿里云，私有云 Openstack 等等，由于LoadBlancer Service深度结合了云平台，所以只能在一些云平台上来使用。

NodePort Service顾名思义，实质上就是通过在集群的每个node上暴露一个端口，然后将这个端口映射到某个具体的service来实现的，虽然每个node的端口有很多(0~65535)，但是由于安全性和易用性(服务多了就乱了，还有端口冲突问题)实际使用可能并不多。

Ingress可以实现使用nginx等开源的反向代理负载均衡器实现对外暴露服务，可以理解Ingress就是用于配置域名转发的一个东西，在nginx中就类似upstream，它与ingress-controller结合使用，通过ingress-controller监控到pod及service的变化，动态地将ingress中的转发信息写到诸如nginx、apache、haproxy等组件中实现方向代理和负载均衡。

* 部署Nginx-ingress-controller
  * RBAC，创建文件：nginx-ingress-rbac.yaml，kubectl apply -f nginx-ingress-rbac.yaml

```
#apiVersion: v1
#kind: Namespace
#metadata:  #这里是创建一个namespace，因为此namespace早有了就不用再创建了
#  name: kube-system
---
apiVersion: v1
kind: ServiceAccount    
metadata:
  name: nginx-ingress-serviceaccount #创建一个serveerAcount
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRole
metadata:
  name: nginx-ingress-clusterrole   #这个ServiceAcount所绑定的集群角色
rules:
  - apiGroups:
      - "" 
    resources:    #此集群角色的权限，它能操作的API资源 
      - configmaps
      - endpoints
      - nodes
      - pods
      - secrets
    verbs:
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - nodes
    verbs:
      - get
  - apiGroups:
      - ""
    resources:
      - services
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - "extensions"
    resources:
      - ingresses
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
        - events
    verbs:
        - create
        - patch
  - apiGroups:
      - "extensions"
    resources:
      - ingresses/status
    verbs:
      - update
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: Role
metadata:         
  name: nginx-ingress-role  #这是一个角色，而非集群角色 
  namespace: kube-system
rules:  #角色的权限 
  - apiGroups:
      - ""
    resources:
      - configmaps
      - pods
      - secrets
      - namespaces
    verbs:
      - get
  - apiGroups:
      - ""
    resources:
      - configmaps
    resourceNames:
      # Defaults to "<election-id>-<ingress-class>"
      # Here: "<ingress-controller-leader>-<nginx>"
      # This has to be adapted if you change either parameter
      # when launching the nginx-ingress-controller.
      - "ingress-controller-leader-nginx"
    verbs:
      - get
      - update
  - apiGroups:
      - ""
    resources:
      - configmaps
    verbs:
      - create
  - apiGroups:
      - ""
    resources:
      - endpoints
    verbs:
      - get
      - create
      - update
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: RoleBinding       #角色绑定
metadata:
  name: nginx-ingress-role-nisa-binding
  namespace: kube-system
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: nginx-ingress-role
subjects:
  - kind: ServiceAccount
    name: nginx-ingress-serviceaccount #绑定在这个用户 
    namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding      #集群绑定
metadata:
  name: nginx-ingress-clusterrole-nisa-binding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: nginx-ingress-clusterrole
subjects:
  - kind: ServiceAccount
    name: nginx-ingress-serviceaccount   #集群绑定到这个serviceacount
    namespace: kube-system   #集群角色是可以跨namespace，但是这里只指明给这个namespce来使用
```

  * defaultbackend，创建文件：nginx-ingress-defaultbackend.yaml，kubectl apply -f nginx-ingress-defaultbackend.yaml, 注意修改label

```
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: default-http-backend
  labels:
    k8s-app: default-http-backend
  namespace: kube-system
spec:
  replicas: 1
  template:
    metadata:
      labels:
        k8s-app: default-http-backend
    spec:
      terminationGracePeriodSeconds: 60
      containers:
      - name: default-http-backend
        # Any image is permissable as long as:
        # 1. It serves a 404 page at /
        # 2. It serves 200 on a /healthz endpoint
        image: registry.cn-hangzhou.aliyuncs.com/google_containers/defaultbackend:1.4
        livenessProbe:
          httpGet:
            path: /healthz   #这个URI是 nginx-ingress-controller中nginx里配置好的localtion 
            port: 8080
            scheme: HTTP
          initialDelaySeconds: 30   #30s检测一次/healthz
          timeoutSeconds: 5
        ports:
        - containerPort: 8080
        resources:
          limits:
            cpu: 10m
            memory: 20Mi
          requests:
            cpu: 10m
            memory: 20Mi
      nodeSelector:            #指定调度到些Node, 以便后面DNS解析
        kubernetes.io/hostname: ali-race-2c8g-node01 
---
apiVersion: v1
kind: Service     #为default backend 创建一个service
metadata:
  name: default-http-backend
  namespace: kube-system
  labels:
    k8s-app: default-http-backend
spec:
  ports:
  - port: 80
    targetPort: 8080
  selector:
    k8s-app: default-http-backend
```

  * controller，创建文件：nginx-ingress-controller.yaml，kubectl apply -f nginx-ingress-controller.yaml, 注意修改label

```
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: nginx-ingress-controller
  labels:
    k8s-app: nginx-ingress-controller
  namespace: kube-system
spec:
  replicas: 1
  template:
    metadata:
      labels:
        k8s-app: nginx-ingress-controller
    spec:
      # hostNetwork makes it possible to use ipv6 and to preserve the source IP correctly regardless of docker configuration
      # however, it is not a hard dependency of the nginx-ingress-controller itself and it may cause issues if port 10254 already is taken on the host
      # that said, since hostPort is broken on CNI (https://github.com/kubernetes/kubernetes/issues/31307) we have to use hostNetwork where CNI is used
      # like with kubeadm
      # hostNetwork: true #注释表示不使用宿主机的80口，
      terminationGracePeriodSeconds: 60
      hostNetwork: true  #表示容器使用和宿主机一样的网络
      serviceAccountName: nginx-ingress-serviceaccount #引用前面创建的serviceacount
      containers:   
      - image: registry.cn-hangzhou.aliyuncs.com/google_containers/nginx-ingress-controller:0.14.0     #容器使用的镜像
        name: nginx-ingress-controller  #容器名
        readinessProbe:   #启动这个服务时要验证/healthz 端口10254会在运行的node上监听。 
          httpGet:
            path: /healthz
            port: 10254
            scheme: HTTP
        livenessProbe:
          httpGet:
            path: /healthz
            port: 10254
            scheme: HTTP
          initialDelaySeconds: 10  #每隔10做健康检查 
          timeoutSeconds: 1
        ports:
        - containerPort: 80  
          hostPort: 80    #80映射到80
        - containerPort: 443
          hostPort: 443
        env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace
        args:
        - /nginx-ingress-controller
        - --default-backend-service=$(POD_NAMESPACE)/default-http-backend
#        - --default-ssl-certificate=$(POD_NAMESPACE)/ingress-secret    #这是启用Https时用的
      nodeSelector:  #指明运行在哪，此IP要和default backend是同一个IP
        kubernetes.io/hostname: ali-race-2c8g-node01  #上面映射到了hostport80，确保此IP80，443没有占用.
```

* 向刚才的dashboard添加ingress配置
  * 注意dashboard要求使用https因此要加入spec.tls.secretName: k8s-dashboard-secret,所以先创建k8s-dashboard-secret
```
# 创建 tls secret
openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout ./tls.key -out ./tls.crt -subj "/CN=your domain"

# 安装 tls secret
kubectl -n kube-system create secret tls k8s-dashboard-secret --key ./tls.key --cert ./tls.crt
```

```
# Copyright 2017 The Kubernetes Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# ------------------- Dashboard Secret ------------------- #

apiVersion: v1
kind: Secret
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard-certs
  namespace: kube-system
type: Opaque

---
# ------------------- Dashboard Service Account ------------------- #

apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard
  namespace: kube-system

---
# ------------------- Dashboard Role & Role Binding ------------------- #

kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: kubernetes-dashboard-minimal
  namespace: kube-system
rules:
  # Allow Dashboard to create 'kubernetes-dashboard-key-holder' secret.
- apiGroups: [""]
  resources: ["secrets"]
  verbs: ["create"]
  # Allow Dashboard to create 'kubernetes-dashboard-settings' config map.
- apiGroups: [""]
  resources: ["configmaps"]
  verbs: ["create"]
  # Allow Dashboard to get, update and delete Dashboard exclusive secrets.
- apiGroups: [""]
  resources: ["secrets"]
  resourceNames: ["kubernetes-dashboard-key-holder", "kubernetes-dashboard-certs"]
  verbs: ["get", "update", "delete"]
  # Allow Dashboard to get and update 'kubernetes-dashboard-settings' config map.
- apiGroups: [""]
  resources: ["configmaps"]
  resourceNames: ["kubernetes-dashboard-settings"]
  verbs: ["get", "update"]
  # Allow Dashboard to get metrics from heapster.
- apiGroups: [""]
  resources: ["services"]
  resourceNames: ["heapster"]
  verbs: ["proxy"]
- apiGroups: [""]
  resources: ["services/proxy"]
  resourceNames: ["heapster", "http:heapster:", "https:heapster:"]
  verbs: ["get"]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: kubernetes-dashboard-minimal
  namespace: kube-system
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: kubernetes-dashboard-minimal
subjects:
- kind: ServiceAccount
  name: kubernetes-dashboard
  namespace: kube-system

---
# ------------------- Dashboard Deployment ------------------- #

kind: Deployment
apiVersion: apps/v1
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard
  namespace: kube-system
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      k8s-app: kubernetes-dashboard
  template:
    metadata:
      labels:
        k8s-app: kubernetes-dashboard
    spec:
      containers:
      - name: kubernetes-dashboard
        image: registry.cn-hangzhou.aliyuncs.com/google_containers/kubernetes-dashboard-amd64:v1.10.1
        ports:
        - containerPort: 8443
          protocol: TCP
        args:
          - --auto-generate-certificates
          # Uncomment the following line to manually specify Kubernetes API server Host
          # If not specified, Dashboard will attempt to auto discover the API server and connect
          # to it. Uncomment only if the default does not work.
          # - --apiserver-host=http://my-address:port
        volumeMounts:
        - name: kubernetes-dashboard-certs
          mountPath: /certs
          # Create on-disk volume to store exec logs
        - mountPath: /tmp
          name: tmp-volume
        livenessProbe:
          httpGet:
            scheme: HTTPS
            path: /
            port: 8443
          initialDelaySeconds: 30
          timeoutSeconds: 30
      volumes:
      - name: kubernetes-dashboard-certs
        secret:
          secretName: kubernetes-dashboard-certs
      - name: tmp-volume
        emptyDir: {}
      serviceAccountName: kubernetes-dashboard
      # Comment the following tolerations if Dashboard must not be deployed on master
      tolerations:
      - key: node-role.kubernetes.io/master
        effect: NoSchedule

---
# ------------------- Dashboard Service ------------------- #

kind: Service
apiVersion: v1
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard
  namespace: kube-system
spec:
  ports:
    - port: 443
      targetPort: 8443
  selector:
    k8s-app: kubernetes-dashboard

---
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: dashboard-ingress
  namespace: kube-system
  annotations:
    nginx.ingress.kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/secure-backends: "true"
    nginx.ingress.kubernetes.io/ssl-passthrough: "true"
spec:
  tls:
   - secretName: k8s-dashboard-secret
  rules:
  - host: dashboard.k8s.clsaa.com
    http:
      paths:
        - backend:
            serviceName: kubernetes-dashboard
            servicePort: 443

```

### 8.1.3.原理介绍

在理解了容器技术之后，你可能已经萌生出了这样一个想法，为什么不用容器部署 Kubernetes 呢？

这样，只要给每个Kubernetes组件做一个容器镜像，然后在每台宿主机上用dockerrun指令启动这些组件容器，部署不就完成了吗？事实上，在 Kubernetes 早期的部署脚本里，确实有一个脚本就是用 Docker 部署 Kubernetes 项目的，这个脚本相比于 SaltStack 等的部署方式，也的确简单了不少。

但是，这样做会带来一个很麻烦的问题，即：如何容器化kubelet。在上一篇文章中，已经提到 kubelet 是 Kubernetes 项目用来操作 Docker 等容器运行时的核心组件。可是，除了跟容器运行时打交道外，kubelet 在配置容器网络、管理容器数据卷时，都需要直接操作宿主机。而如果现在 kubelet 本身就运行在一个容器里，那么直接操作宿主机就会变得很麻烦。对于网络配置来说还好，kubelet 容器可以通过不开启 Network Namespace（即 Docker 的 host network 模式）的方式，直接共享宿主机的网络栈。

可是，要让 kubelet 隔着容器的 Mount Namespace 和文件系统，操作宿主机的文件系统，就有点儿困难了。比如，如果用户想要使用 NFS 做容器的持久化数据卷，那么 kubelet 就需要在容器进行绑定挂载前，在宿主机的指定目录上，先挂载 NFS 的远程目录。可是，这时候问题来了。由于现在 kubelet 是运行在容器里的，这就意味着它要做的这个“mount 一 F nfs”命令，被隔离在了一个单独的 Mount Namespace 中。即，kubelet 做的挂载操作，不能被“传播”到宿主机上。

对于这个问题，有人说，可以使用 setns（）系统调用，在宿主机的 Mount Namespace 中执行这些挂载操作；也有人说，应该让 Docker 支持一个-mnt=host 的参数。

但是，到目前为止，在容器里运行 kubelet，依然没有很好的解决办法，也不推荐你用容器去部署 Kubernetes 项目。

正因为如此，kubeadm 选择了一种妥协方案：把 kubelet 直接运行在宿主机上，然后使用容器部署其他的 Kubernetes 组件。所以，你使用 kubeadm 的第-一步，是在机器上手动安装 kubeadm、kubelet 和 kubectl 这三个二进制文件。当然，kubeadm 的作者已经为各个发行版的 Linux 准备好了安装包，所以你只需要执行：

```shell

$ yum install kubeadm

```

### 8.1.4.kubeadm init的工作流程

当执行 kubeadm init 指令后，kubeadm 首先要做的，是一系列的检査工作，以确定这台机器可以用来部署 Kubernetes。这一步检查，我们称为“Preflight Checks“，它可以为你省掉很多后续的麻烦。

其实，Preflight Checks 包括了很多方面，比如:

1. Linux 内核的版本必须是否是 3.10 以上？Linux Groups 模块是否可用？
2. 机器的 hostname 是否标准？在 Kubernetes 项目里，机器的名字以及一切存储在 Etcd 中的 AP이I 对象，都必须使用标准的 DNS 命名（RFC1123) 用户安装的 kubeadm 和 kubelet 的版本是否匹配？机器上是不是已经安装了 Kubernetes的二进制文件?
3. Kubernetes的工作端口10250/10251/10252 端口是不是已经被占用？ip、mount 等 Linux 指令是否存在？Docker 是否已经安装
4. 在通过了 Preflight Checks 之后，kubeadm 要为你做的，是生成 Kubernetes 对外提供服务所噐的各种证书和对应的目录

Kubernetes 对外提供服务时，除非专门开启“不安全模式”，否则都要通过 HTTPS 才能访问 kube- pserver。这就需要为 Kubernetes 集群配置好证书文件。

kubeadm 为 Kubernetes项目生成的证书文件都放在 Master节点的/etc/ kubernetes/pki 目录下。在这个目录下，最主要的证书文件是 ca. Ct 和对应的私钥 ca. Key。

此外，用户使用 kubectl 获取容器日志等 streaming 操作时，需要通过 kube- pserver 向 kubelet 发起请求，这个连接也必须是安全的。kubeadm 为这一步生成的是 pserver- kubelet client. Crt 文件，对应的私钥是 pserver- kubelet- client. Key

除此之外，Kubernetes 集群中还有 Aggregate Aplserver 等特性，也需要用到专门的证书，这里就不再一一列举了。需要指出的是，你可以选择不让 kubeadm 为你生成这些证书，而是拷贝现有的证书到如下证书的目录里:

```
/etc/kubernetes/pki/ca.{crt,key}
```

这时，kubeadm 就会跳过证书生成的步骤，把它完全交给用户处理证书生成后，kubeadm 接下来会为其他组件生成访问 kube- pserver 所需的配置文件。这些文件的路径是：/etc/ kubernetes/xxx.conf

```
ls /etc/kubernetes/
admin.conf  controller-manager.conf  kubelet.conf  scheduler.conf
```

这些文件里面记录的是，当前这个 Master 节点的服务器地址、监听端口、证书目录等信息。这样，对应的客户端（比如 scheduler, kubelet 等），可以直接加载相应的文件，使用里面的信息与 kube- pserver 建立安全连接

接下来，kubeadm 会为 Master 组件生成 Pod 配置文件。已经在上一篇文章中和你介绍过 Kubernetes 有三个 Master 组件 kube- pserver、kube- controller- manager、kube scheduler，而它我们都会被使用 Pod 的方式部署起来。

你可能会有些疑问：这时，Kubernetes 集群尚不存在，难道 kubeadm 会直接执行 docker run 来启动这些容器吗?当然不是。

在 Kubernetes 中，有一种特殊的容器启动方法叫做“Static Pod”。它允许你把要部署的 Pod 的 YAML 文件放在一个指定的目录里。这样，当这台机器上的 kubelet 启动时，它会自动检查这个目录，加载所有的 Pod YAML 文件，然后在这台机器上启动它我们。

从这一点也可以看出，kubelet 在 Kubernetes 项目中的地位非常高，在设计上它就是一个完全独立的组件，而其他 Master 组件，则更像是辅助性的系统容器。

在 kubeadm中, Master 组件的YAML 文件会被生成在/etc/ kubernetes/ manifests 路径下。比 3 n, kube-apiserver yaml

```
apiVersion: v1
kind: Pod
metadata:
  annotations:
    scheduler.alpha.kubernetes.io/critical-pod: ""
  creationTimestamp: null
  labels:
    component: kube-apiserver
    tier: control-plane
  name: kube-apiserver
  namespace: kube-system
spec:
  containers:
  - command:
    - kube-apiserver
    - --authorization-mode=Node,RBAC
    - --runtime-config=api/all=true
    - --advertise-address=10.168.0.2
    ...
    - --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
    - --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
    image: k8s.gcr.io/kube-apiserver-amd64:v1.11.1
    imagePullPolicy: IfNotPresent
    livenessProbe:
      ...
    name: kube-apiserver
    resources:
      requests:
        cpu: 250m
    volumeMounts:
    - mountPath: /usr/share/ca-certificates
      name: usr-share-ca-certificates
      readOnly: true
    ...
  hostNetwork: true
  priorityClassName: system-cluster-critical
  volumes:
  - hostPath:
      path: /etc/ca-certificates
      type: DirectoryOrCreate
    name: etc-ca-certificates
  ...
```

关于ー个 Pod 的 YAML 文件怎么写、里面的字段如何解读，会在后续专门的文章中为你详细分析。在这里，你只需要关注这样几个信息:

1. 这个 Pod 里只定义了ー个容器，它使用的镜像是: k8s.gcr,io/kube- pserver amd64: v1.11.1。这个镜像是 Kubernetes 官方维护的一个组件镜像。
2. 这个容器的启动命令（commands）是 kube- pserver-- authorization-mode-Node, RBAC这样一句非常长的命令。其实，它就是容器里 kube- pserver 这个二进制文件再加上指定的配置参数而已
3. 如果你要修改一个已有集群的 kube- pserver 的配置，需要修改这个 YAML 文件
4. 这些组件的参数也可以在部署时指定，很快就会讲解到

在这一步完成后，kubeadm 还会再生成一个 Etcd 的 Pod YAML 文件，用来通过同样的 Static Pod 的方式启动 Etcd。所以，最后 Master 组件的 Pod YAML 文件如下所示:

```
$ ls /etc/kubernetes/manifests/
etcd.yaml  kube-apiserver.yaml  kube-controller-manager.yaml  kube-scheduler.yaml
```

而一旦这些 YAML 文件出现在被 kubelet监视的/etc/ kubernetes/ manifests 目录下，kubelet 就会自动创建这些 YAML 文件中定义的 Pod，即 Master 组件的容器。

Master 容器启动后，kubeadm 会通过检查 localhost:6443/ health 这个 Master 组件的健康检查 URL，等待 Master 组件完全运行起来

然后，kubeadm 就会为集群生成一个 bootstrap token。在后面，只要持有这个 token，任何一个安装了 kubelet 和 kuban 的节点，都可以通过 kubeadm join 加入到这个集群当中

这个 token 的值和使用方法会，会在 kubeadm init 结束后被打印出来。

在 token 生成之后，kubeadm 会将 ca. Crt 等 Master 节点的重要信息，通过 Configmap 的方式保存在 Etcd 当中，供后续部署 Node 节点使用。这个 Confgmap 的名字是 cluster-info。

kubeadm init 的最后一步，就是安装默认插件。Kubernetes 默认 kube- proxy 和 DNS 这两个插件是必须安装的。它我们分别用来提供整个集群的服务发现和 DNS 功能。其实，这两个插件也只是两个容器镜像而已，所以 kubeadm 只要用 Kubernetes 客户端创建两个 Pod 就可以了.


### 8.1.5.kubeadm join的工作流程

这个流程其实非常简单，kubeadm init 生成 bootstrap token 之后，你就可以在任意一台安装了 kubelet 和 kubeadm 的机器 H 执行 kubeadm join 了。

可是，为什么执行 kubeadm join 需要这样一个 token 呢？

因为，任何一台机器想要成为 Kubernetes 集群中的一个节点，就必须在集群的 kube apiserver 上注册。可是，要想跟 apiserver 打交道，这台机器就必须要获取到相应的证书文件（CA 文件）。可是，为了能够一键安装，我们就不能让用户去 Master 节点上手动拷贝这些文件。

所以，kubeadm 至少需要发起一次“不安全模式”的访问到 kube -apiserver，从而拿到保存在 ConfigMap 中的 cluster-info（它保存了 APIServer 的授权信息）。而 bootstrap token，扮演的就是这个过程中的安全验证的角色。

只要有了 cluster-info 里的 kube- apiserver 的地址、端口、证书，kubelet 就可以以“安全模式”连接到 apiserver 上，这样一个新的节点就部署完成了。

接下来，你只要在其他节点上重复这个指令就可以了。


### 8.1.6.配置kubeadm的部署参数

在前面讲解了 kubeadm 部署 Kubernetes 集群最关键的两个步骤，kubeadm init 和 kubeadm join。相信你一定会有这样的疑问：kubeadm 确实简单易用，可是又该如何定制的集群组件参数呢？

比如，要指定 kube-apiserver 的启动参数，该怎么办？

推荐使用 kubeadm init 部署 Master 节点时，使用下面这条指令：

```
$ kubeadm init --config kubeadm.yaml
```

给 kubeadm 提供一个 YAML 文件:

```
apiVersion: kubeadm.k8s.io/v1alpha2
kind: MasterConfiguration
kubernetesVersion: v1.11.0
api:
  advertiseAddress: 192.168.0.102
  bindPort: 6443
  ...
etcd:
  local:
    dataDir: /var/lib/etcd
    image: ""
imageRepository: k8s.gcr.io
kubeProxy:
  config:
    bindAddress: 0.0.0.0
    ...
kubeletConfiguration:
  baseConfig:
    address: 0.0.0.0
    ...
networking:
  dnsDomain: cluster.local
  podSubnet: ""
  serviceSubnet: 10.96.0.0/12
nodeRegistration:
  criSocket: /var/run/dockershim.sock
  ...

```

通过制定这样一个部署参数配置文件，你就可以很方便地在这个文件里填写各种自定义的部署参数了。比如，现在要指定 kube- apiserver 的参数，那么只要在这个文件里加上这样一段信息：

```
...
apiServerExtraArgs:
  advertise-address: 192.168.0.103
  anonymous-auth: false
  enable-admission-plugins: AlwaysPullImages,DefaultStorageClass
  audit-log-path: /home/johndoe/audit.log

```

然后，kubeadm 就会使用上面这些信息替换/etc/kubernetes/manifests/kube apiserver. Yaml 里的 command 字段里的参数了。

而这个 YAML 文件提供的可配置项远不止这些。比如，你还可以修改 kubelet 和 kube- proxy 的配置，修改 Kubernetes 使用的基础镜像的 URL（默认的 k8s. Gcr. io/xxx镜像URL 在国内访问是有困难的），指定自己的证书文件，指定特殊的容器运行时等等。这些配置项，就留给你在后续实践中探索了。