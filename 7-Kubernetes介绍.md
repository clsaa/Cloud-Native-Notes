# 7. Kubernetes介绍

## 7.1 Docker与CoreOS的恩怨情仇

* 2013年2月， Docker建立了一个网站发布它的首个演示版本， 3月， 美国加州Alex Polvi正在自己的车库开始他的第二次创业
* 有了第一桶金的Alex这次准备干一票大的，他计划开发一个足以颠覆传统的服务器系统的Linux发行版。为了提供能够从任意操作系统版本稳定无缝地升级到最新版系统的能力， Alex急需解决应用程序与操作系统之间的耦合问题。因此，当时还名不见经传的Docker容器引起了他的注意，凭着敏锐直觉， Alex预见了这个项目的价值，当仁不让地将Docker做为了这个系统支持的第一套应用程序隔离方案。不久以后，他们成立了以自己的系统发行版命名的组织： CoreOS。事实证明，采用Docker这个决定，后来很大程度上成就了CoreOS的生态系统。
* Docker一开始的时候是作为一个组件来构建平台，一个构建块，可以将它分层置入系统来利用容器…这是支撑
* Docker的原始价值，是一个帮助构建东西的简单工具，我认为这是目前它如此成功的原因。
* 但是，Polvi明显觉得，Docker忽略了自己的核心，期望拥有更多功能——成为一个平台。
* Docker 刚问世就红透半边天，不仅拿了融资，还得到了Google 等巨头的支持。 CoreOS此前一直忙于为
* 他们一直认为 Docker 应该成为一个简单的基础单元，但不幸的是事情并如他们期望的那样， Docker正在构建一些工具用于发布云服务器、集群系统以及构建、运行、上传和下载映像等服务，甚至包括底层网络的功能等，以打造自己的Docker平台或生态圈。
* Docker 刚问世就红透半边天，不仅拿了融资，还得到了Google 等巨头的支持。 CoreOS此前一直忙于为Docker 提供技术支持服务
* Alex Polvi 认为，由于 Docker 貌似已经从原本做"业界标准容器"的初心转变成打造一款以容器为中心的企业服务平台， CoreOS 才决定开始推出自己的标准化产品
* Polvi 表示， Docker 在安全性和可组合性方面是有根本上的缺陷的，而 Rocket 的设计原型就是为了弥补这些缺陷。
* CoreOS 的联合创始人兼 CTO Brandon Philips 是 Dcoker 管理委员会的成员， CoreOS 打算继续支持 Docker项目，但是当 Rocket 逐渐成熟之后，他们将重新评估是否继续参与贡献。
* 2015年5月4日举行的CoreOS Fest大会上， CoreOS宣布了它的新合作伙伴： Red Hat、 Google、 Vmware及Apcera。 Docker对容器的标准控制比较独断，而且目前看上去没有引入其他人一起进行设计的可能。 CoreOS的Rocket自定义程度相对更高，对于Google等公司来说，也是可以制衡Docker的手段。所以Google等公司支持Rocket
* 当开始做 CoreOS 的时候，我们便着手构建并传递给大家 Google 的基础架构。如今，伴随着 Tectonic 的诞生，这个目标也实现了。企业可以像 Google 运行其基础设施一样，在世界各地安全运行基于容器的分布式应用。

>cgroup最初由google的工程师提出，后来被整合进Linux内核中而后的Android操作系统也就凭借着这个技术，为每个应用程序分配不同的cgroup，将每个程序进行隔离，达到了一个应用程序不会影响其他应用程序环境的目的。Linux容器正是业界一直关注的Google基础设施Borg和Omega的基础之一，基于之前Google开源的cgroup项目。

```yaml
Borg
这是一个生产环境的经验下的论文,自于Google
高层次的
•任何东西都运行在Borg之中，包含存储系统如CFS和BigTable
•中等类型的集群大小有10k左右的节点，尽管有的要大的多
•节点可以是十分的异构
•使用了Linux的进程隔离（本质上来说是容器），因为Borg出现在现在的虚拟机基础设施之前。效率和启动时间
当时十分重要。
•所有的作业都是静态的链接的可执行文件。
•有非常复杂，十分丰富的资源定义语言可用。
•可以滚动升级运行的作业，这意味着配置和执行文件。这有时需要任务重启，因而容错是很重要的。
```

* 2013年10月3日 - Google发布了自己所用Linux容器系统的开源版本lmctfy
* Docker的负责人之一Solomon Hykes初步探索了代码后认为， lmctfy的一些代码功能非常底层，可以作为独立的库，有可能与Docker等容器项目配合使用。由于代码很干净，占用小，构建体验优秀，他正在考虑将其作为Docker的后端。
* 2015 Docker con大会上，当Polvi和Hykes在台上握手并宣布，启动开放容器基金会，这被认为原有的分裂得到了修复。 Hykes介绍了OCP项目，旨在为容器提供一种通用runtime
* Open Container Project，目标是实现容器镜像格式与运行时的标准化。这很有可能实现， 项目成员包括Docker、CoreOS、 Redhat、 IBM、 Google 、 微软、 IBM、 Google、英特尔、 Amazon、 HP、华为、思科、 EMC 等。
* 我们开发容器APP的经验将为我们合作OCP（开放容器项目）规范起到关键作用。我们期望大多数容器App能直接集成到OCP规范中，稍微做点调整就可以和现存的Docker生态圈完美兼容。最终目标是致力于形成一份统一的容器标准格式规范，而且OCP的成功将意味着容器App的大体目标是令人满意的。 ——Polvi

## 7.2 Kubernetes王者归来

* 2014年6月：谷歌宣布kubernetes 开源。
* 2014年7月： Mircrosoft、 Red Hat、 IBM、 Docker、 CoreOS、 Mesosphere 和Saltstack 加入kubernetes。
* 2014年8月： 2014年8月 ： Mesosphere宣布将kubernetes作为frame整合到mesosphere生态系统中，用于Docker容器集群的调度、部署和管理
* 2014年8月： VMware加入kubernetes社区， Google产品经理 Craig Mcluckie公开表示， VMware将会帮助kubernetes实现利用虚拟化来保证物理主机安全的功能模式。
* 2014年11月 ： HP加入kubernetes 社区。
* 2014年11月： Google容器引擎Alpha启动，谷歌宣布GCE中支持容器及服务，并以kubernetes为构架。
* 2015年1月： Google和Mirantis及伙伴将kubernetes引入OpenStack， 开发者可以在openstack上部署运行kubernetes 应用。
* 2015年4月： Google和CoreOs联合发布Tectonic， 它将kubernetes和CoreOS软件栈整合在了一起。
* 2015年5月： Intel加入kubernetes社区，宣布将合作加速Tectonic软件栈的发展进度。
* 2015年6月： Google容器引擎进入beta版。
* 2015年7月： Google正式加入OpenStack基金会， Kubernetes的产品经理Craig McLuckie宣布Google将成为OpenStack基金会的发起人之一， Google将把它容器计算的专家技术带入OpenStack,成一体提高公有云和私有云的互用性。
* 2015年7月： Kuberentes v1.0正式发布。
* 2015年7月22日Google正式对外发布 Kubernetes v 1.0，与此同时，谷歌联合linux基金会及其他合作伙伴共同成立了CNCF基金会( Cloud Native Computing Foundation)， 并将kuberentes 作为首个编入CNCF管理体系的开源项目，助力容器技术生态的发展进步。
* Kubernete在github开源社区，经过400多位贡献者一年的努力，多达14000次提交，最终达到了在之前会议中确定的发布版本v1的功能要求

## 7.3.新一代PaaS

作为一名开发者，其实并不关心容器运行时的差异。因为，在整个“开发一测试一发布”的流程中，真正承载着容器信息进行传递的，是容器镜像，而不是容器运行时。

这个重要假设，正是容器技术圈在 Docker 项目成功后不久，就迅速走向了“容器编排”这个“上层建筑”的主要原因：作为一家云服务商或者基础设施提供商，我只要能够将用户提交的 Docker 镜像以容器的方式运行起来，就能成为这个非常热闹的容器生态图上的一个承载点，从而将整个容器技术栈上的价值，沉淀在我的这个节点上。

更重要的是，只要从我这个承载点向 Docker 镜像制作者和使用者方向回溯，整条路径。上的各个服 务节点，比如CI/CD、监控、安全、网络、存储等等，都有我可以发挥和盈利的余地。这个逻辑，正是所有云计算提供商如此热衷于容器技术的重要原因：通过容器镜像，它们可以和潜在用户（即，开发者）直接关联起来。

从一个开发者和单一的容器镜像，到无数开发者和庞大的容器集群，容器技术实现了从“容器”到“容器云”的飞跃，标志着它真正得到了市场和生态的认可。这样，容器就从一个开发者手里的小工具，一跃成为了云计算领域的绝对主角；而能够定义容器组织和管理规范的“容器编排'技术，则当仁不让地坐上了容器技术领域的“头把交椅”。

跟很多基础设施领域先有工程实践、后有方法论的发展路线不同，Kubernetes 项目的理论基础则要比工程实践走得靠前得多，这当然要归功于 Google 公司在 2015 年 4 月发布的 Borg 论文了。

Borg 系统，一直以来都被誉为 Google 公司内部最强大的“秘密武器”。虽然略显夸张，但这个说法倒不算是吹牛。

因为，相比于 Spanner、BigTable 等相对。上层的项目，Borg 要承担的责任，是承载 Google 公司整个基础设施的核心依赖。在 Google 公司已经公开发表的基础设施体系论文中，Borg 项目当仁不让地位居整个基础设施技术栈的最底层。

![image](https://static001.geekbang.org/resource/image/c7/bd/c7ed0043465bccff2efc1a1257e970bd.png)

上面这幅图，来自于 Google Omega 论文的第一作者的博士毕业论文。它描绘了当时 Google 已经公开发表的整个基础设施栈。在这个图里，你既可以找到 MapReduce、BigTable 等知名项目，也能看到 Borg 和它的继任者 Omega 位于整个技术栈的最底层。

正是由于这样的定位，Borg 可以说是 Google 最不可能开源的一个项目。而幸运地是，得益于 Docker 项目和容器技术的风靡，它却终于得以以另一种方式与开源社区见面，这个方式就是 Kubernetes 项目。


## 7.4.Kubernetes解决什么问题

希望Kubernetes项目带来的体验是确定的：现在我有了应用的容器镜像，请帮我在一个给定的集群。上把这个应用运行起来。

还希望 Kubernetes 能给我提供路由网关、水平扩展、监控、备份、灾难恢复等一系列运维能力。

![image](https://static001.geekbang.org/resource/image/8e/67/8ee9f2fa987eccb490cfaa91c6484f67.png)

我们可以看到，Kubernetes 项目的架构，跟它的原型项目 Borg 非常类似，都由 Master 和 Node两种节点组成，而这两种角色分别对应着控制节点和计算节点其中，控制节点，即 Master 节点，由三个紧密协作的独立组件组合而成，它们分别是负责 API 服务的 kube- pserver、负责调度的 kube- scheduler，以及负责容器编排的 kube- controller manager。整个集群的持久化数据，则由 kube- pserver 处理后保存在 Ectd 中。而计算节点上最核心的部分，则是一个叫作 kubelet 的组件。

在 Kubernetes 项目中，kubelet 主要负责同容器运行时（比如 Docker 项目）打交道。而这个交互所依赖的，是一个称作 CRNI (Container Runtime Interface）的远程调用接口，这个接口定义了容器运行时的各项核心操作，比如：启动一个容器需要的所有参数。这也是为何，Kubernetes 项目并不关心你部署的是什么容器运行时、使用的什么技术实现，只要你的这个容器运行时能够运行标准的容器镜像，它就可以通过实现 CRI 接入到 Kubernetes 项目当中而具体的容器运行时，比如 Docker 项目，则一般通过 O이这个容器运行时规范同底层的 Linux 操作系统进行交互，即：把 CR 请求翻译成对 Linux 操作系统的调用（操作 Linux Namespace 和 Groups 等）

此外，kubelet 还通过 gRPC 协议同一个叫作 Device Plugin 的插件进行交互。这个插件，是 Kubernetes 项目用来管理 GPU 等宿主机物理设备的主要组件，也是基于 Kubernetes 项目进行机器学习训练、高性能作业支持等工作必须关注的功能而 kubelet 的另一个重要功能，则是调用网络插件和存储插件为容器配置网络和持久化存储。这两个插件与 kubelet 进行交互的接口，分别是 CNI (Container Networking Interface）和 CSI  (Container Storage Interface)

那么，Borg 对于 Kubernetes 项目的指导作用又体现在哪里呢？答案是，Master 节点。

虽然在 Master 节点的实现细节。上 Borg 项目与 Kubernetes 项目不尽相同，但它们的出发点却高度一致，即：如何编排、管理、调度用户提交的作业？所以，Borg 项目完全可以把 Docker 镜像看做是一种新的应用打包方式。这样，Borg 团队过去在大规模作业管理与编排。上的经验就可以直接“套”在 Kubernetes 项目上了。这些经验最主要的表现就是，从一开始，Kubernetes 项目就没有像同时期的各种“容器云”项目那样，把 Docker 作为整个架构的核心，而仅仅把它作为最底层的一个容器运行时实现。而 Kubernetes 项目要着重解决的问题，则来自于 Borg 的研究人员在论文中提到的一个非常重要的观点：运行在大规模集群中的各种任务之间，实际上存在着各种各样的关系。这些关系的处理，才是作业编排和管理系统最困难的地方。事实也正是如此。其实，这种任务与任务之间的关系，在我们平常的各种技术场景中随处可见。比如，一个 Web 应用与数据库之间的访问关系，一个负载均衡器和它的后端服务之间的代理关系，一个门户应用与授权组件之间的调用关系。更进一步地说，同属于一个服务单位的不同功能之间，也完全可能存在这样的关系。比如，一个 Web 应用与日志搜集组件之间的文件交换关系。


可是，如果我们现在的需求是，要求这个项目能够处理前面提到的所有类型的关系，甚至还要能够支持未来可能出现的更多种类的关系呢？

这时，“link“这种单独针对-种案例设计的解决方案就太过简单了。如果你做过架构方面的工作，就会深有感触：一旦要追求项目的普适性，那就一-定要从顶层开始做好设计。

所以，Kubernetes 项目最主要的设计思想是，从更宏观的角度，以统-的方式来定义任务之间的各种关系，并且为将来支持更多种类的关系留有余地。

比如，Kubernetes 项目对容器间的“访问”进行了分类，首先总结出了一类非常常见的“紧密交互”的关系，即：这些应用之间需要非常频繁的交互和访问；又或者，它们会直接通过本地文件进行信息交换。

在常规环境下，这些应用往往会被直接部署在同一台机器。上，通过 Localhost 通信，通过本地磁盘目录交换文件。而在 Kubernetes 项目中，这些容器则会被划分为一个“Pod”，Pod 里的容器共享同一个 Network Namespace、同一组数据卷，从而达到高效率交换信息的目的。

Pod 是 Kubernetes 项目中最基础的一个对象，源自于 Google Borg 论文中一个名叫 Alloc 的设计。在后续的章节中，我们会对 Pod 做更进一步地阐述。

而对于另外-种更为常见的需求，比如 Web 应用与数据库之间的访问关系，Kubernetes 项目则提供了一种叫作“Service”的服务。像这样的两个应用，往往故意不部署在同一台机器上，这样即使 Web 应用所在的机器宕机了，数据库也完全不受影响。可是，我们知道，对于一个容器来说，它的 IP 地址等信息不是固定的，那么 Web 应用又怎么找到数据库容器的 Pod 呢？所以，Kubernetes 项目的做法是给 Pod 绑定一个 Service 服务，而 Service 服务声明的 IP 地址等信息是“终生不变”的。这个 Service 服务的主要作用，就是作为 Pod 的代理入口（Portal），从而代替 Pod 对外暴露一个固定的网络地址。这样，对于 Web 应用的 Pod 来说，它需要关心的就是数据库 Pod 的 Service 信息。不难想象，Service 后端真正代理的 Pod 的 IP 地址、端口等信息的自动更新、维护，则是 Kubernetes 项目的职责。像这样，围绕着容器和 Pod 不断向真实的技术场景扩展，我们就能够摸索出- -幅如下所示的 Kubernetes 项目核心功能的“全景图”。

![image](https://static001.geekbang.org/resource/image/16/06/16c095d6efb8d8c226ad9b098689f306.png)

按照这幅图的线索，我们从容器这个最基础的概念出发，首先遇到了容器间“紧密协作”关系的难题，于是就扩展到了 Pod；有了 Pod 之后，我们希望能一次启动多个应用的实例，这样就需要 Deployment 这个 Pod 的多实例管理器；而有了这样-组相同的 Pod 后，我们又需要通过一个固定的 IP 地址和端口以负载均衡的方式访问它，于是就有了 Service。可是，如果现在两个不同 Pod 之间不仅有“访问关系”，还要求在发起时加上授权信息。最典型的例子就是 Web 应用对数据库访问时需要 Credential（数据库的用户名和密码）信息。那么，在 Kubernetes 中这样的关系又如何处理呢？Kubernetes 项目提供了一种叫作 Secret 的对象，它其实是一个保存在 Etcd 里的键值对数据。这样，你把 Credential 信息以 Secret 的方式存在 Etcd 里，Kubernetes 就会在你指定的 Pod（比如，Web 应用的 Pod）启动时，自动把 Secret 里的数据以 Volume 的方式挂载到容器里。这样，这个 Web 应用就可以访问数据库了。

除了应用与应用之间的关系外，应用运行的形态是影响“如何容器化这个应用”的第二个重要因素。

为此，Kubernetes 定义了新的、基于 Pod 改进后的对象。比如 Job，用来描述一次性运行的 Pod（比如，大数据任务）；再比如 DaemonSet，用来描述每个宿主机。上必须且只能运行一个副本的守护进程服务；又比如 CronJob，则用于描述定时任务等等。如此种种，正是 Kubernetes 项目定义容器间关系和形态的主要方法。可以看到，Kubernetes 项目并没有像其他项目那样，为每一个管理功能创建一个指令，然后在项目中实现其中的逻辑。这种做法，的确可以解决当前的问题，但是在更多的问题来临之后，往往会力不从心。相比之下，在 Kubernetes 项目中，我们所推崇的使用方法是：

首先，通过一个“编排对象”，比如 Pod、Job、CronJob 等，来描述你试图管理的应用；

然后，再为它定义一些“服务对象”，比如 Service、Secret、Horizontal Pod Autoscaler（自动水平扩展器）等。这些对象，会负责具体的平台级功能。

这种使用方法，就是所谓的“声明式 API”。这种 API 对应的“编排对象”和“服务对象”，都是 Kubernetes 项目中的 API 对象（API Object).

这就是 Kubernetes 最核心的设计理念